\documentclass[a4paper,UKenglish,cleveref, autoref,mathscr]{lipics-v2019}
%This is a template for producing LIPIcs articles.
%See lipics-manual.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
%for section-numbered lemmas etc., use "numberwithinsect"
%for enabling cleveref support, use "cleveref"
%for enabling cleveref support, use "autoref"

\usepackage{bbm, tikz, mathtools, thm-restate}
\usetikzlibrary{arrows,calc,automata,intersections}
\tikzset{LMC style/.style={>=angle 60,every edge/.append style={thick},every state/.style={thick,minimum size=20,inner sep=0.5}}}

\usepackage{asymptote}

\newcommand{\eow}{\$}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\Epsilon}{\mathcal{E}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\e}{\mathrm{e}}
\newcommand{\Bor}{\mathscr{B}}
\newcommand{\BorC}{\mathscr{C}}
\newcommand{\GG}{\mathscr{G}}
\newcommand{\HH}{\mathscr{H}}
\newcommand{\Norm}{\mathscr{N}}
%\newcommand{\FF}{\mathscr{F}}
\newcommand{\DD}{\mathscr{D}}
\newcommand{\LL}{\mathscr{L}}
\newcommand{\Exp}{\mathit{Exp}}
\newcommand{\MM}{\mathscr{M}}
\newcommand{\Basis}{\mathscr{B}}
\newcommand{\balph}{\boldsymbol{\alpha}}
\newcommand{\bpldP}{\boldsymbol{P}}
\newcommand{\stoch}{\boldsymbol{S}}
\newcommand{\TT}{\boldsymbol{T}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\pem}{\mathbf{A}}
\newcommand{\con}{\textbf{con}}
\newcommand{\Con}{\overline{\textbf{con}}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\pl}{\Gamma_{\mathit{GEM}}}
\newcommand{\Leb}{\lambda_{\mathit{Leb}}}

\graphicspath{ {images/} }

\DeclareMathOperator{\Span}{span\,}

%\newcommand{\stefan}[1]{\marginpar{\textcolor{blue}{#1}}}

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Equivalence of Hidden Markov Models with Continuous Observations}

\author{Oscar Darwin}{Department of Computer Science, Oxford University, United Kingdom }{}{https://orcid.org/0000-0001-5016-014X}{}%TODO mandatory, please use full name; only 1 author per \author macro; first two parameters are mandatory, other parameters can be empty. Please provide at least the name of the affiliation and the country. The full address is optional

\author{Stefan Kiefer}{Department of Computer Science, Oxford University, United Kingdom}{}{https://orcid.org/0000-0003-4173-6877}{}

\authorrunning{O. Darwin and S. Kiefer}%TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{John Q. Public and Joan R. Public}%TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

%\ccsdesc[100]{General and reference~General literature}
%\ccsdesc[100]{General and reference}%TODO mandatory: Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm
\ccsdesc[500]{Theory of computation~Random walks and Markov chains}
\ccsdesc[500]{Mathematics of computing~Stochastic processes}
\ccsdesc[300]{Theory of computation~Logic and verification}

\keywords{Markov chains, equivalence, probabilistic systems, verification}%TODO mandatory; please add comma-separated list of keywords

\category{}%optional, e.g. invited paper

\relatedversion{}%optional, e.g. full version hosted on arXiv, HAL, or other respository/website
%\relatedversion{A full version of the paper is available at \url{...}.}

\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...

%\funding{(Optional) general funding statement \dots}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

%\acknowledgements{I want to thank \dots}%optional

%\nolinenumbers %uncomment to disable line numbering

%\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\sloppy
\begin{document}

\maketitle

\begin{abstract}
Lyapunov
\end{abstract}

\section{Introduction}


\section{Preliminaries}
We write $\NN$ for the set of positive integers, $\QQ$ for the set of rationals and $\QQ_+$ for the set of positive rationals.
For $d \in \NN$ and a finite set $Q$ we use the notation $|Q|$ for the number of elements in $Q$, $[d] = \{1, \dots, d\}$ and $[Q] = \{1, \dots, |Q|\}$. Vectors $\mu \in \RR^N$ are viewed as row vectors and we write $\1 = (1, \dots, 1) \in \RR^N$.
Superscript~$T$ denotes transpose; e.g., $\1^T$ is a column vector of ones.
A matrix $M \in \RR^{N \times N}$ is \emph{stochastic} if $M$ is non-negative and $\sum_{j = 1}^{N} M_{i,j} = 1$ for all $i \in [N]$.
For a domain $\Sigma$ and subset $E \subset \Sigma$ the \emph{characteristic} function $\chi_E : \Sigma \rightarrow \{0,1\}$ is defined as $\chi_E(x) = 1$ if $x \in E$ and $\chi_E(x) = 0$ otherwise. Given a function $\gamma : [N] \rightarrow S \subseteq [N]$ and a matrix $M \in \RR^{N \times N}$ clearly $S$ is isomorphic to $[M]$ for some $M \leq N$ by a unique monotonically increasing isomorphism $\phi : S \rightarrow [M]$. We may define the restricted matrix $(M\restriction_S)_{i,j} = M_{\gamma^{-1 } \circ \phi^{-1} (i), \gamma^{-1 } \circ \phi^{-1} (j)}$.

Throughout this paper, we use $\Sigma$ to denote a set of \emph{observations}.
We assume $\Sigma$ is a topological space and $(\Sigma, \GG, \lambda)$ is a measure space where every open subset $E \in \GG$ has non-zero measure. Indeed $\RR$ and the usual Lebesgue measure space on $\RR$ satisfy these assumptions.
The set $\Sigma^n$ is the set of words over~$\Sigma$ of length $n$ and $\Sigma^* = \bigcup_{n = 0}^\infty \Sigma^n$.


A matrix valued function $\Psi : \Sigma \rightarrow [0,\infty)^{N \times N}$ can be integrated element-wise.
We write $\int_E \Psi\,d\lambda$ for the matrix with entries $\left( \int_E \Psi\, d\lambda \right)_{i,j} = \int_E \Psi_{i,j}\, d\lambda$, where $\Psi_{i,j} : \Sigma \rightarrow [0,\infty)$ is defined by $\Psi_{i,j}(x) = \big( \Psi(x) \big)_{i,j}$ for all $x \in \Sigma$.


A function $f : \Sigma \rightarrow \RR^m$ is \emph{piecewise continuous} if there is an open set $C \subset \Sigma$, called a \emph{set of continuity}, such that $f$ is continuous on $C$ and for every point $x \in  \Sigma \setminus C$ there is some sequence of points $x_n \in C$ such that $\lim_{n \rightarrow \infty} x_n = x$ and $\lim_{n \rightarrow \infty} f(x_n) = f(x)$. For a non-negative function $f : \Sigma \rightarrow [0,\infty)$ we use the notation ${\supp~f = \{x \in \Sigma \mid f(x) > 0\}}$.


\begin{definition}\label{HMMdef}
A \emph{Hidden Markov Model} (HMM) is a triple $(Q, \Sigma, \Psi)$ where $Q$ is a finite set of states, $\Sigma$ is a set of observations, and the \emph{observation density matrix} $\Psi : \Sigma \rightarrow [0,\infty)^{|Q| \times |Q|}$ specifies the transitions such that $\int_\Sigma \Psi\, d\lambda$ is a stochastic matrix.
\end{definition}
\begin{example} \label{ex-HMMdef}
The HMM from the introduction is the triple $(\{q_1, q_2\}, \mathbb{R}, \Psi)$ with
\begin{equation}
\Psi(x) \ = \ \begin{pmatrix}
\frac12 \cdot 2 \exp(-2 x) \cdot \chi_{[0,\infty)}(x) && \frac12 \cdot 1 \cdot \chi_{[-1,0)}(x) \\
\frac13 \cdot \frac12 \cdot \chi_{[0,2)}(x) && \frac23 \cdot \exp(-x) \cdot \chi_{[0,\infty)}(x)
\end{pmatrix}\,. \tag*{\qed}
\end{equation}
\end{example}
We assume that $\Psi$ is piecewise continuous and extend $\Psi$ to the mapping $\Psi : \Sigma^* \rightarrow [0,\infty)^{|Q| \times |Q|}$ with $\Psi(x_1 \cdots x_n) = \Psi(x_1) \times \dots \times \Psi(x_n)$ for $x_1, \dots, x_n \in \Sigma$. If $C$ is the set of continuity for $\Psi : \Sigma \rightarrow [0,\infty)^{|Q| \times |Q|}$, then for fixed $n \in \NN$ the restriction $\Psi : \Sigma^n \rightarrow [0,\infty)^{|Q| \times |Q|}$ is piecewise continuous with set of continuity $C^n$. We say that $A \subseteq \Sigma^n$ is a \emph{cylinder set} if $A = A_1 \times \dots \times A_n$ and $A_i \in \GG$ for $i \in [n]$. For every $n$ there is an induced measure space $(\Sigma^n, \GG^n, \lambda^n)$ where $\GG^n$ is the smallest $\sigma$-algebra containing all cylinder sets in~$\Sigma^n$ and $\lambda^n(A_1 \times \dots \times A_n) = \prod_{i = 1}^n \lambda(A_i)$ for any cylinder set $A_1 \times \dots \times A_n$. Let $A \subset \Sigma^n$ and write $A \Sigma^\omega$ for the set of infinite words over~$\Sigma$ where the first $n$ observations fall in the set~$A$. Given a HMM $(Q, \Sigma, \Psi)$ and initial distribution $\pi$ on $Q$ viewed as vector $\pi \in \RR^{|Q|}$, there is an induced probability space $(\Sigma^\omega, \GG^*, \PP_\pi)$ where $\Sigma^\omega$ is the set of infinite words over~$\Sigma$, and $\GG^*$ is the smallest $\sigma$-algebra containing (for all $n \in \NN$) all sets $A \Sigma^\omega$ where $A\subseteq \Sigma^n$ is a cylinder set and $\PP_\pi$ is the unique probability measure such that
$\PP_\pi(A \Sigma^\omega) =  \pi \int_A \Psi\, d\lambda^n \1^T$
for any cylinder set $A \subseteq \Sigma^n$.
\begin{definition}
For two distributions $\pi_1$ and $\pi_2$ and a HMM $C = (Q, \Sigma, \Psi)$, we say that $\pi_1$ and~$\pi_2$ are \emph{equivalent}, written $\pi_1 \equiv_C \pi_2$, if $\PP_{\pi_1}(A) = \PP_{\pi_2}(A)$ holds for all measurable subsets $A \subseteq \Sigma^\omega$.
\end{definition}
One could define equivalence of two pairs $(C_1,\pi_1)$ and $(C_2,\pi_2)$ where $C_i = (Q_i, \Sigma, \Psi_i)$ are HMMs and $\pi_i$ are initial distributions for $i=1,2$.
We do not need that though, as we can define, in a natural way, a single HMM over the disjoint union of $Q_1$ and~$Q_2$ and consider instead equivalence of $\pi_1$ and~$\pi_2$ (where $\pi_1,\pi_2$ are appropriately padded with zeros).

%A key concept used throughout this paper is that of \emph{functional decomposition}.
Given an observation density matrix $\Psi$, a \emph{functional decomposition} consists of functions $f_k : \Sigma \rightarrow [0,\infty)$ and matrices $P_k \in \RR^{|Q| \times |Q|}$ for $k \in [d]$ such that $\Psi(x) = \sum_{k = 1}^d f_k(x) P_k$ for all $x \in \Sigma$ and $\int_{\Sigma} f_k\, d\lambda = 1$ for all $k \in [d]$. We sometimes abbreviate this decomposition as $\Psi = \sum_{k = 1}^d f_k P_k$ and this notion has a central role in our paper.

\begin{example} \label{ex-functional-decomposition}
The observation density matrix~$\Psi$ from \cref{ex-HMMdef} has a functional decomposition
\begin{align*}
\Psi(x) \ = \
& 2 \exp(-2 x) \chi_{[0,\infty)}(x)
\begin{pmatrix}
\frac12 && 0 \\ 0 && 0
\end{pmatrix}
+
\chi_{[-1,0)}(x)
\begin{pmatrix}
0 && \frac12 \\ 0 && 0
\end{pmatrix}
+ \mbox{} \\
& \frac12 \chi_{[0,2)}(x)
\begin{pmatrix}
0 && 0 \\
\frac13 && 0
\end{pmatrix}
+
\exp(-x) \chi_{[0,\infty)}(x)
\begin{pmatrix}
0 && 0 \\
0 && \frac23
\end{pmatrix}
\,. \tag*{\qed}
\end{align*}
\end{example}

\begin{lemma}\label{stochasticPk}
Let $(Q, \Sigma, \Psi)$ be a HMM.
If $\Psi$ has functional decomposition $\Psi =  \sum_{k = 1}^d f_k P_k$ then $\sum_{k = 1}^d P_k$ is stochastic.
\end{lemma}
\begin{proof}
By definition of a HMM, $\int_{\Sigma} \Psi\, d\lambda$ is stochastic, and we have
\begin{equation*}
\int_{\Sigma} \Psi\, d\lambda = \int_{\Sigma} \sum_{k = 1}^d f_k P_k\, d\lambda =  \sum_{k = 1}^d P_k  \int_{\Sigma} f_k\, d\lambda =  \sum_{k = 1}^d P_k.\qedhere
\end{equation*}
\end{proof}
When $\Sigma$ is finite, it follows that $\int_\Sigma \Psi\, d\lambda = \sum_{a \in \Sigma} \Psi(a)$. Hence $\sum_{a \in \Sigma} \Psi(a)$ is stochastic.

\subparagraph*{Encoding}
For computational purposes we assume that rational numbers are represented as ratios of integers in binary.
The initial distribution of a HMM with state set~$Q$ is given as a vector $\pi \in \QQ^{|Q|}$.
We also need to encode continuous functions, in particular, density functions such as Gaussian, exponential or piecewise-polynomial functions.
A \emph{profile} is a finite word (i.e., string) that describes a continuous function.
It may consist of (an encoding of) a function type and its parameters. For example, the profile  $(\mathcal{N}, \mu, \sigma)$ may denote a Gaussian (also called normal) distribution with mean $\mu \in \QQ$ and standard deviation $\sigma \in \QQ_+$. A profile may also consist of a description of a rational linear combination of such building blocks.
For any profile~$\gamma$ we write $[\![\gamma]\!] : \Sigma \rightarrow [0,\infty)$ for the function it encodes.
For example, a profile $\gamma = (\mathcal{N}, \mu, \sigma)$ with $\mu \in \QQ,\ \sigma \in \QQ_+$ may encode the function $[\![\gamma]\!] : \RR \rightarrow [0,\infty)$ given as $[\![\gamma]\!](x) = \frac{1}{\sigma\sqrt{2\pi}} \exp{- \frac{(x - \mu)^2}{2\sigma^2}}$.
Without restricting ourselves to any particular encoding, we assume that $\Gamma$ is a \emph{profile language}, i.e., a finitely presented but usually infinite set of valid profiles. For any $\Gamma_0 \subseteq \Gamma$ we write $[\![\Gamma_0]\!] = \{[\![\gamma]\!] \mid \gamma \in \Gamma_0\}$.

We use profiles to encode HMMs $C = (Q, \Sigma, \Psi)$:
we say that $C$ is \emph{over}~$\Gamma$ if the observation density matrix~$\Psi$ is given as a matrix of pairs $(p_{i,j}, \gamma_{i,j}) \in \QQ_+  \times \Gamma$ such that $\Psi_{i,j} = p_{i,j} [\![\gamma_{i,j}]\!]$ and $\int_{\Sigma} [\![\gamma_{i,j}]\!]\,d\lambda = 1$ hold for all $i,j \in [Q]$. In this way the $p_{i,j}$ form the transition probabilities of the embedded Markov chain and the $\gamma_{i,j}$ encode the probability densities of the observations upon each transition.

\begin{example} \label{ex-encoding-prelims}
For a suitable profile language~$\Gamma$, the HMM from \cref{ex-HMMdef} may be over~$\Gamma$, with the observation density matrix given as
\begin{equation}
\begin{pmatrix}
(\frac12, (\Exp,2)) && (\frac12, (U,-1,0)) \\
(\frac13, (U,0,2))  && (\frac23, (\Exp,1))
\end{pmatrix}\,. \tag*{\qed}
\end{equation}
\end{example}
The observation density matrix~$\Psi$ of a HMM $(Q, \Sigma, \Psi)$ with \emph{finite}~$\Sigma$ can be given as a
list of matrices $\Psi(a) \in \QQ_+^{|Q| \times |Q|}$ for all $a \in \Sigma$ such that $\sum_{a \in \Sigma} \Psi(a)$ is a stochastic matrix.

\section{Convergence to Lyapunov Exponent}

Let $(\Sigma^\omega, \GG^*, \mu)$ be a probability space on infinite words and let $M = (Q, \Sigma, \Psi)$ be an HMM. We define the equivalence relation on $[Q]$ as $i \sim_\mu j$ if and only if $\forall i, j \in [Q], v \in \Sigma^*$ there exists a word $w \in \Sigma^*$ such that $\mu(v w \Sigma^\omega) > 0$ and $\Psi(w)_{i,j} > 0$. $\sim_\mu$ defines a partition $P$ on the states of $Q$ and if $P = \{Q\}$ then we say $M$ is $\mu$-connected. Clearly if $\pi$ is an initial distribution for $M$ and $\PP_{\pi}$ the resulting measure on infinite words, then $M$ is ($\PP_\pi$-)connected if and only if $\int_\Sigma \Psi d\lambda$ is an irreducible stochastic matrix.

\begin{lemma}
Let $(Q, \Sigma, \Psi)$ be a connected HMM and let $\pi$ be an initial distribution. Then $\int_\Sigma \Psi d\lambda$ has a stationary distribution $\mu$ with $\pi << \mu$ and therefore for any event $E \in \GG^*$, $\mu(E) = 1$ implies $\pi(E) = 1$.  
\end{lemma}

\begin{proof}
The stochastic matrix $\int_\Sigma \Psi d\lambda$ has a left eigenvector $\mu$ for the eigenvalue $1$ since $\int_\Sigma \Psi d\lambda \1^T = \1^T$. For any $i,j \in [Q]$ such that $\mu_i > 0$ by irredicibility and non-negativity, there is a $k \in \NN$ such that $(\int_\Sigma \Psi d\lambda)^k_{i,j} > 0$ and therefore $\mu_j = (\mu (\int_\Sigma \Psi d\lambda)^k)_j > 0$. Therefore, $\mu$ has full support and there is some $\alpha > 0$ such that $\pi \leq \alpha \mu$. Let $E \in \GG^*$ then there is a sequence of events $E_n \in \GG^n$ such that
\begin{align}
\pi(E) & = \lim_{n \rightarrow \infty} \pi(E_n) \\
& = \pi \big( \lim_{n \rightarrow \infty}\int_{E_n} \Psi d\lambda \big) \1^T \\
& \leq \alpha \mu \big( \lim_{n \rightarrow \infty}\int_{E_n} \Psi d\lambda \big) \1^T \\
& = \alpha \mu(E).
\end{align} 
Therefore it follows that $\pi << \mu$ and in particular $\mu(E) = 1 \implies \mu(E^c) = 0 \implies \pi(E^c) = 0 \implies \pi(E) = 1$.
\end{proof}

\subsection{Ergodicity}
Let $(\Sigma^\omega, \GG^*)$ be a measurable space of infinite words. The left-shift operator $\LL : \Sigma^\omega \rightarrow \Sigma^\omega$ is given as $\LL(w)_i = w_{i + 1}$. We say that a measure $\PP$ on $(\Sigma^\omega, \GG^*)$ is ergodic for $\LL$ if the following two conditions hold
\begin{itemize}
\item $\PP(\LL^{-1}(E)) = \PP(E) \quad \forall E \in \GG^*$
\item $\lim_{k \rightarrow \infty} \frac{1}{k} \sum_{i = 0}^{k  - 1}\PP_\mu(\LL^{-i}(A) \cap B) = \PP_\mu(A) \PP_\mu(B) \quad \forall A, B \in \GG^*$.
\end{itemize}

\begin{lemma}\label{markovergodic}
Let $M$ be an irreducible stochastic matrix, then $\lim_{k \rightarrow \infty} \frac{1}{k} \sum_{i = 0}^{k - 1} M^i = \1^T \mu$ where $\mu$ is the stationary distribution of $M$.
\end{lemma}


\begin{lemma}\label{ergodichmmleftshift}
Let $(Q, \Sigma, \Psi)$ be a connected HMM and let $\mu$ be an stationary distribution for $\int_\Sigma \Psi d\lambda$ then $\PP_\mu$ is ergodic for $\LL$.
\end{lemma}

\begin{proof}
For both conditions by \cite[p.~20, p.~41]{wal81} it suffices to check just cylinder sets. Let $n \in \NN$ and $E \in \Sigma^n$. Clearly, 
\[\PP_\mu(\LL^{-1}(E)) = \mu \int_{\Sigma}\Psi d\lambda \int_{E}\Psi d\lambda^n \1^T = \mu \int_{E}\Psi d\lambda^n \1^T = \PP_\mu(E).\]
And similarly, let $A, B \in \Sigma^n$ then
\begin{align*}
\lim_{k \rightarrow \infty} \frac{1}{k} \sum_{i = 0}^{k  - 1}\PP_\mu(\LL^{-i}(A) \cap B) & = \lim_{k \rightarrow \infty} \frac{1}{k} \sum_{i = n}^{k  - 1}\PP_\mu(\LL^{-i}(A) \cap B) \\
& = \lim_{k \rightarrow \infty} \frac{1}{k} \sum_{i = n}^{k  - 1} \mu \int_{B}\Psi d\lambda^n \int_{\Sigma}\Psi d\lambda^{i - n} \int_{A}\Psi d\lambda^n \1^T \\
& = \mu \int_{B}\Psi d\lambda^n \Big[ \lim_{k \rightarrow \infty} \frac{1}{k} \sum_{i = n}^{k  - 1} \Big( \int_{\Sigma}\Psi d\lambda \Big)^{i - n} \Big] \int_{A}\Psi d\lambda^n \1^T \\
& = \mu \int_{B}\Psi d\lambda^n \1^T \mu \int_{A}\Psi d\lambda^n \1^T \\
& = \PP_\mu(A)\PP_\mu(B)
\end{align*}
where the fourth equality follows by \Cref{markovergodic}.
\end{proof}

\begin{theorem}\textsc{\textbf{Kingman's Subadditive Ergodic Theorem}}
Let $(\Sigma^\omega, \GG^*, \mu)$ a probability space on infinite words such that $\mu$ is an ergodic measure for $\LL$. Further, let $X_n$ be a sequence of integrable random variables on this space such that $X_{n+m}(w) \leq X_{n}(w) + X_{m}(\LL^n w)$. Then $\lim_{n \rightarrow \infty} \frac{X_n}{n} $ converges $\mu$-a.e. to a constant.
\end{theorem}

\begin{corollary}\label{kingmancor}
Let $M = (Q_1, \Sigma, \Psi)$ be an HMM and suppose $\mu$ is an ergodic measure for $\LL$. Then, $\frac{1}{n} \ln \| \Psi_n \|$ converges $\mu$-a.e. to a constant.
\end{corollary}

\begin{proof}
$\LL$ is ergodic for measure $\mu$ by \Cref{ergodichmmleftshift}. Writing $w_n$ for the $n$-th letter of the infinite word $w$ and $X_n = \ln \| \Psi(w_1 \cdots w_{n}) \|$ it follows that $X_n$ is integrable and

\begin{align*}
X_{n + m}(w) = \ln \| \Psi(w_1 \cdots w_{n + m}) \| & \leq \ln \| \Psi(w_1 \cdots w_n) \| \|\Psi(w_{n+1} \cdots w_{n + m}) \| \\
& = \ln \| \Psi(w_1 \cdots w_n) \| + \ln \|\Psi(w_{n+1} \cdots w_{n + m}) \| \\
& = X_n(w) + X_{m}(T^n w).  
\end{align*}
\end{proof}


\begin{theorem}\textsc{\textbf{Osedelet's Theorem}}
Let $C_n : \Sigma^\omega \rightarrow \RR^{r \times r}$ satisfy
\begin{itemize}
\item $C_0(w) = I \quad \forall w \in \Sigma^\omega$
\item $C_{n + m}(w) = C_{m}(\LL^n(w))C_n(w)$
\end{itemize}
\end{theorem}


\begin{theorem}\label{lyplimit}
Suppose $\mu$ is an ergodic measure for $\LL$ and let $C_n : \Sigma^\omega \rightarrow \RR^{r \times r}$ be a cocycle. Suppose that for all $i,j \in [r]$ and $v \in \Sigma^*$ such that $\mu(v) > 0$ there exists a word $w \in \Sigma^*$ such that $\mu(vw) > 0$ and and $C_n(vw)_{i,j}$ let $\pi$ be an initial distribution. Then
\[
\frac{1}{n} \ln \big( \| \pi C_n(w_n) \| \big) \rightarrow \lambda \quad \mu-\text{a.s.}
\]
where $\lambda \in [-\infty, 0]$ and $w_n \in \Sigma^n$ is the length $n$ prefix of a word $w \in \Sigma^\omega$. 
\end{theorem}

\begin{proof}
By Osedelet's theorem, since $\mu$ is ergodic, $\frac{1}{n} \ln \big( \| \pi C(w_n) \| \big)$ converges $\mu$-a.s to a constant depending on $\pi$. Let $e_i$ denote the characteristic vector of state $i$. Without loss of generality, assume $e_1 \leq \supp ~\pi$ then by $\mu$-connectivity we may construct a sequence of words $v_1, \dots v_{|Q|}$ such that $e_{i + 1} \leq \supp ~ e_i \Psi(v_i)$ and $\mu(v_1 \cdots v_{|Q|} \Sigma^\omega) > 0$ where the indices are taken mod $|Q|$. It follows that $\1 = \supp ~ \sum_{i = 1}^{|Q|} \pi \Psi(v_1 \cdots v_i)$. Noting by equivalence of norms in finite dimensional spaces that $\frac{1}{n} \ln \big( \| \1 \Psi(w_n) \| \big)$ converges to a constant by \Cref{kingmancor}, it follows that 
\begin{align*}
\lim_{n \rightarrow \infty} \frac{1}{n} \ln \big( \| \1 \Psi(w_n) \| \big) & = \lim_{n \rightarrow \infty} \frac{1}{n} \ln \big( \| \sum_{i = 1}^{|Q|} \pi \Psi(v_1 \cdots v_i) \Psi(w_n) \| \big) \\
& \leq \lim_{n \rightarrow \infty} \frac{1}{n} \ln \big( |Q| \max_{i \in [Q]} \| \pi \Psi(v_1 \cdots v_i) \Psi(w_n) \| \big) \\ 
& = \max_{i \in [Q]} \lim_{n \rightarrow \infty} \frac{1}{n} \ln \big( \| \pi \Psi(v_1 \cdots v_i) \Psi(w_n) \| \big) \\
& \leq \lim_{n \rightarrow \infty} \frac{1}{n} \ln \big( \| \1 \Psi(w_n) \| \big).
\end{align*}
The third equality follows by the existance of individual limits by Osedelet's theorem and the fourth inequality follows since $\supp ~ u \leq \1$ for any $|Q|$ dimensional vector $u$. In particular the sequence of inequalities above implies that there is some word $v_1 \cdots v_i$ such that 
\[\mu \Big\{ \lim_{n\rightarrow\infty}\frac{1}{n} \ln \big( \| \pi \Psi(w_n) \| \big) = \lim_{n \rightarrow \infty} \frac{1}{n} \ln \big( \| \1 \Psi(w_n) \| \big) \mid v_1 \cdots v_i\Sigma^\omega \Big\} = 1\]
which in turn implies that $\mu \Big\{ \lim_{n\rightarrow\infty}\frac{1}{n} \ln ( \| \pi \Psi(w_n) \| ) = \lim_{n \rightarrow \infty} \frac{1}{n} \ln ( \| \1 \Psi(w_n) \| ) \Big\} > 0$ and hence by Osedelet's theorem must be $\mu$-a.s.
\end{proof}

Let $M = (Q, \Sigma, \Phi)$ be an HMM then given $S \subseteq Q$ we may define the restricted alphabet $\Sigma_{S} = \{a \in \Sigma \mid \Phi(a)\restriction_S \neq 0\}$.


Now consider two HMMs $M_1 = (Q_1, \Sigma, \Psi^1)$ and $M_2 = (Q_2, \Sigma, \Psi^2)$ then we define the equivalence relation $\sim$ on $Q_1 \times Q_2$ as $(i_1, i_2) \sim (j_1, j_2)$ if and only if there are words $w, u \in \Sigma^*$ such that $\Psi_r(w)_{i_r, j_r} > 0$ and $\Psi_r(u)_{j_r, i_r} > 0$ for $r = 1, 2$. Let $\mathcal{P}_{\bigotimes}$ denote the partition defined by $\sim$. Write $l : Q_1 \times Q_2 \rightarrow Q_1$ and $r : Q_1 \times Q_2 \rightarrow Q_2$ for the left and right projection functions defined in the usual way. 

\begin{theorem}
For each $P \in \mathcal{P}_{\bigotimes}$ the HMM $M_2^P = (r(P), \Sigma_{r(P)}, \Psi \restriction r(P))$ is well defined and connected with a stationary distribution $\mu_P$. Further, the matrix valued function defined by restricting the domain of $\Phi^1 \restriction l(P)$ to $\Sigma_{r(P)}$ forms a cocycle when extended to $\Sigma_{r(P)}^*$ in the usual way.




Further, $M_1^P$ is $\mu_P$-connected. By \Cref{lyplimit} $\frac{1}{n} \ln \big( \| \pi \Psi_{\Sigma_{r(P)}, l(P)}(w_n) \| \big)$ converges $\mu_P$ almost surely to a constant $\lambda_P$. For any two initial distributions $\pi_1$ and $\pi_2$ on $Q_1, Q_2$ respectively
\[
\PP_{\pi_2}(\frac{1}{n} \ln \big( \| \pi_1 \Psi_1(w_n) \| \big) = \lambda_P) = \PP(\text{hitting } P \text{ in } \sum_{a \in \Sigma} \Psi^1 \bigotimes \Psi^2 (a) \text{ from } \pi_1 \times \pi_2).
\]
\end{theorem}

\begin{proof}

\end{proof}





\begin{corollary}
Let $(Q, \Sigma, \Psi)$ be an HMM with embedded Markov matrix $M = \sum_{a \in \Sigma} \Psi(a)$. Consider the bottom connected components of $M$ given as a disjoint family of sets $\cP$ such that $\bigcup_{P \in \cP} P \subseteq Q$. Then, $(P, \Sigma, \Psi_P)$ is a connected HMM. By \Cref{lyplimit} the limit $$\frac{1}{n} \ln \big( \pi \Psi_P(w_n) \1^T \big) \rightarrow \lambda_P$$
exists $\mu$-a.s. Writing $q_P$ for the hitting probability of $P$ under $M$, the limit
$$\frac{1}{n} \ln \big( \pi \Psi(w_n) \1^T \big)$$
converges $\mu$-a.s to a random variable $X_\infty$ with $\mu(X_\infty = \lambda_P) = q_P$. 
\end{corollary}


\begin{corollary}
Let $\pi_1$ and $\pi_2$ be initial distributions for a HMM $(Q, \Sigma, \Psi)$ and write $L_n$ for the likelihood ratio $\frac{\pi_1 \Psi(w_n) \1^T}{\pi_2 \Psi(w_n) \1^T}$. Then $\frac{1}{n} \ln L_n$ converges $\mu$-a.s to a random variable on  with finite support.  	
\end{corollary}


\bibliography{lyapunovhmm}

\appendix


\end{document}
