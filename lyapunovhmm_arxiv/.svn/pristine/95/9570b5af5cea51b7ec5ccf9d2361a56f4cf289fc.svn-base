\documentclass[a4paper,UKenglish,cleveref, autoref,mathscr]{lipics-v2019}
%This is a template for producing LIPIcs articles.
%See lipics-manual.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
%for section-numbered lemmas etc., use "numberwithinsect"
%for enabling cleveref support, use "cleveref"
%for enabling cleveref support, use "autoref"

\usepackage{bbm, tikz, mathtools, thm-restate}
\usetikzlibrary{arrows,calc,automata,intersections}
\tikzset{LMC style/.style={>=angle 60,every edge/.append style={thick},every state/.style={thick,minimum size=20,inner sep=0.5}}}

\usepackage{asymptote}

\newcommand{\eow}{\$}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\Epsilon}{\mathcal{E}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\e}{\mathrm{e}}
\newcommand{\Bor}{\mathscr{B}}
\newcommand{\BorC}{\mathscr{C}}
\newcommand{\GG}{\mathscr{G}}
\newcommand{\HH}{\mathscr{H}}
\newcommand{\Norm}{\mathscr{N}}
%\newcommand{\FF}{\mathscr{F}}
\newcommand{\DD}{\mathscr{D}}
\newcommand{\LL}{\mathscr{L}}
\newcommand{\Exp}{\mathit{Exp}}
\newcommand{\MM}{\mathscr{M}}
\newcommand{\Basis}{\mathscr{B}}
\newcommand{\balph}{\boldsymbol{\alpha}}
\newcommand{\bpldP}{\boldsymbol{P}}
\newcommand{\stoch}{\boldsymbol{S}}
\newcommand{\TT}{\boldsymbol{T}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\pem}{\mathbf{A}}
\newcommand{\con}{\textbf{con}}
\newcommand{\Con}{\overline{\textbf{con}}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\pl}{\Gamma_{\mathit{GEM}}}
\newcommand{\MLeb}{\MM_{\mathit{Leb}}}
\newcommand{\lyapexp}{\lim_{n\rightarrow\infty} \frac1n \ln \pi_1 \Psi(w) \1^T}
\newcommand{\liexp}{\lim_{n\rightarrow\infty} \frac1n \ln L_n}
\newcommand{\PPind}{\PP_{\text{ind}}}

\graphicspath{ {images/} }

\DeclareMathOperator{\Span}{span\,}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

%\newcommand{\stefan}[1]{\marginpar{\textcolor{blue}{#1}}}

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Equivalence of Hidden Markov Models with Continuous Observations}

\author{Oscar Darwin}{Department of Computer Science, Oxford University, United Kingdom }{}{https://orcid.org/0000-0001-5016-014X}{}%TODO mandatory, please use full name; only 1 author per \author macro; first two parameters are mandatory, other parameters can be empty. Please provide at least the name of the affiliation and the country. The full address is optional

\author{Stefan Kiefer}{Department of Computer Science, Oxford University, United Kingdom}{}{https://orcid.org/0000-0003-4173-6877}{}

\authorrunning{O. Darwin and S. Kiefer}%TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{John Q. Public and Joan R. Public}%TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

%\ccsdesc[100]{General and reference~General literature}
%\ccsdesc[100]{General and reference}%TODO mandatory: Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm
\ccsdesc[500]{Theory of computation~Random walks and Markov chains}
\ccsdesc[500]{Mathematics of computing~Stochastic processes}
\ccsdesc[300]{Theory of computation~Logic and verification}

\keywords{Markov chains, equivalence, probabilistic systems, verification}%TODO mandatory; please add comma-separated list of keywords

\category{}%optional, e.g. invited paper

\relatedversion{}%optional, e.g. full version hosted on arXiv, HAL, or other respository/website
%\relatedversion{A full version of the paper is available at \url{...}.}

\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...

%\funding{(Optional) general funding statement \dots}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

%\acknowledgements{I want to thank \dots}%optional

%\nolinenumbers %uncomment to disable line numbering

%\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\sloppy
\begin{document}

\maketitle

\begin{abstract}
Lyapunov
\end{abstract}

\section{Introduction}


\section{Preliminaries}
We write $\NN$ for the set of positive integers, $\QQ$ for the set of rationals and $\QQ_+$ for the set of positive rationals.
For $d \in \NN$ and a finite set $Q$ we use the notation $|Q|$ for the number of elements in $Q$, $[d] = \{1, \dots, d\}$ and $[Q] = \{1, \dots, |Q|\}$. Vectors $\mu \in \RR^N$ are viewed as row vectors and we write $\1 = (1, \dots, 1) \in \RR^N$.
Superscript~$T$ denotes transpose; e.g., $\1^T$ is a column vector of ones.
A matrix $M \in \RR^{N \times N}$ is \emph{stochastic} if $M$ is non-negative and $\sum_{j = 1}^{N} M_{i,j} = 1$ for all $i \in [N]$.
For a domain $\Sigma$ and subset $E \subset \Sigma$ the \emph{characteristic} function $\chi_E : \Sigma \rightarrow \{0,1\}$ is defined as $\chi_E(x) = 1$ if $x \in E$ and $\chi_E(x) = 0$ otherwise. Given a function $\gamma : [N] \rightarrow S \subseteq [N]$ and a matrix $M \in \RR^{N \times N}$ clearly $S$ is isomorphic to $[M]$ for some $M \leq N$ by a unique monotonically increasing isomorphism $\phi : S \rightarrow [M]$. We may define the restricted matrix $(M\restriction_S)_{i,j} = M_{\gamma^{-1 } \circ \phi^{-1} (i), \gamma^{-1 } \circ \phi^{-1} (j)}$.

Throughout this paper, we use $\Sigma$ to denote a set of \emph{observations}.
We assume $\Sigma$ is a topological space and $(\Sigma, \GG, \lambda)$ is a measure space where every open subset $E \in \GG$ has non-zero measure. Indeed $\RR$ and the usual Lebesgue measure space on $\RR$ satisfy these assumptions.
The set $\Sigma^n$ is the set of words over~$\Sigma$ of length $n$ and $\Sigma^* = \bigcup_{n = 0}^\infty \Sigma^n$.


A matrix valued function $\Psi : \Sigma \rightarrow [0,\infty)^{N \times N}$ can be integrated element-wise.
We write $\int_E \Psi\,d\lambda$ for the matrix with entries $\left( \int_E \Psi\, d\lambda \right)_{i,j} = \int_E \Psi_{i,j}\, d\lambda$, where $\Psi_{i,j} : \Sigma \rightarrow [0,\infty)$ is defined by $\Psi_{i,j}(x) = \big( \Psi(x) \big)_{i,j}$ for all $x \in \Sigma$.


A function $f : \Sigma \rightarrow \RR^m$ is \emph{piecewise continuous} if there is an open set $C \subset \Sigma$, called a \emph{set of continuity}, such that $f$ is continuous on $C$ and for every point $x \in  \Sigma \setminus C$ there is some sequence of points $x_n \in C$ such that $\lim_{n \rightarrow \infty} x_n = x$ and $\lim_{n \rightarrow \infty} f(x_n) = f(x)$. For a non-negative function $f : \Sigma \rightarrow [0,\infty)$ we use the notation ${\supp~f = \{x \in \Sigma \mid f(x) > 0\}}$.


\begin{definition}\label{HMMdef}
A \emph{Hidden Markov Model} (HMM) is a triple $(Q, \Sigma, \Psi)$ where $Q$ is a finite set of states, $\Sigma$ is a set of observations, and the \emph{observation density matrix} $\Psi : \Sigma \rightarrow [0,\infty)^{|Q| \times |Q|}$ specifies the transitions such that $\int_\Sigma \Psi\, d\lambda$ is a stochastic matrix.
\end{definition}
\begin{example} \label{ex-HMMdef}
The HMM from the introduction is the triple $(\{q_1, q_2\}, \mathbb{R}, \Psi)$ with
\begin{equation}
\Psi(x) \ = \ \begin{pmatrix}
\frac12 \cdot 2 \exp(-2 x) \cdot \chi_{[0,\infty)}(x) && \frac12 \cdot 1 \cdot \chi_{[-1,0)}(x) \\
\frac13 \cdot \frac12 \cdot \chi_{[0,2)}(x) && \frac23 \cdot \exp(-x) \cdot \chi_{[0,\infty)}(x)
\end{pmatrix}\,. \tag*{\qed}
\end{equation}
\end{example}
We assume that $\Psi$ is piecewise continuous and extend $\Psi$ to the mapping $\Psi : \Sigma^* \rightarrow [0,\infty)^{|Q| \times |Q|}$ with $\Psi(x_1 \cdots x_n) = \Psi(x_1) \times \dots \times \Psi(x_n)$ for $x_1, \dots, x_n \in \Sigma$. If $C$ is the set of continuity for $\Psi : \Sigma \rightarrow [0,\infty)^{|Q| \times |Q|}$, then for fixed $n \in \NN$ the restriction $\Psi : \Sigma^n \rightarrow [0,\infty)^{|Q| \times |Q|}$ is piecewise continuous with set of continuity $C^n$. We say that $A \subseteq \Sigma^n$ is a \emph{cylinder set} if $A = A_1 \times \dots \times A_n$ and $A_i \in \GG$ for $i \in [n]$. For every $n$ there is an induced measure space $(\Sigma^n, \GG^n, \lambda^n)$ where $\GG^n$ is the smallest $\sigma$-algebra containing all cylinder sets in~$\Sigma^n$ and $\lambda^n(A_1 \times \dots \times A_n) = \prod_{i = 1}^n \lambda(A_i)$ for any cylinder set $A_1 \times \dots \times A_n$. Let $A \subset \Sigma^n$ and write $A \Sigma^\omega$ for the set of infinite words over~$\Sigma$ where the first $n$ observations fall in the set~$A$. Given a HMM $(Q, \Sigma, \Psi)$ and initial distribution $\pi$ on $Q$ viewed as vector $\pi \in \RR^{|Q|}$, there is an induced probability space $(\Sigma^\omega, \GG^*, \PP_\pi)$ where $\Sigma^\omega$ is the set of infinite words over~$\Sigma$, and $\GG^*$ is the smallest $\sigma$-algebra containing (for all $n \in \NN$) all sets $A \Sigma^\omega$ where $A\subseteq \Sigma^n$ is a cylinder set and $\PP_\pi$ is the unique probability measure such that
$\PP_\pi(A \Sigma^\omega) =  \pi \int_A \Psi\, d\lambda^n \1^T$
for any cylinder set $A \subseteq \Sigma^n$.
\begin{definition}
For two distributions $\pi_1$ and $\pi_2$ and a HMM $C = (Q, \Sigma, \Psi)$, we say that $\pi_1$ and~$\pi_2$ are \emph{equivalent}, written $\pi_1 \equiv_C \pi_2$, if $\PP_{\pi_1}(A) = \PP_{\pi_2}(A)$ holds for all measurable subsets $A \subseteq \Sigma^\omega$.
\end{definition}
One could define equivalence of two pairs $(C_1,\pi_1)$ and $(C_2,\pi_2)$ where $C_i = (Q_i, \Sigma, \Psi_i)$ are HMMs and $\pi_i$ are initial distributions for $i=1,2$.
We do not need that though, as we can define, in a natural way, a single HMM over the disjoint union of $Q_1$ and~$Q_2$ and consider instead equivalence of $\pi_1$ and~$\pi_2$ (where $\pi_1,\pi_2$ are appropriately padded with zeros).

%A key concept used throughout this paper is that of \emph{functional decomposition}.
Given an observation density matrix $\Psi$, a \emph{functional decomposition} consists of functions $f_k : \Sigma \rightarrow [0,\infty)$ and matrices $P_k \in \RR^{|Q| \times |Q|}$ for $k \in [d]$ such that $\Psi(x) = \sum_{k = 1}^d f_k(x) P_k$ for all $x \in \Sigma$ and $\int_{\Sigma} f_k\, d\lambda = 1$ for all $k \in [d]$. We sometimes abbreviate this decomposition as $\Psi = \sum_{k = 1}^d f_k P_k$ and this notion has a central role in our paper.

\subparagraph*{Encoding}
For computational purposes we assume that rational numbers are represented as ratios of integers in binary.
The initial distribution of a HMM with state set~$Q$ is given as a vector $\pi \in \QQ^{|Q|}$.
We also need to encode continuous functions, in particular, density functions such as Gaussian, exponential or piecewise-polynomial functions.
A \emph{profile} is a finite word (i.e., string) that describes a continuous function.
It may consist of (an encoding of) a function type and its parameters. For example, the profile  $(\mathcal{N}, \mu, \sigma)$ may denote a Gaussian (also called normal) distribution with mean $\mu \in \QQ$ and standard deviation $\sigma \in \QQ_+$. A profile may also consist of a description of a rational linear combination of such building blocks.
For any profile~$\gamma$ we write $[\![\gamma]\!] : \Sigma \rightarrow [0,\infty)$ for the function it encodes.
For example, a profile $\gamma = (\mathcal{N}, \mu, \sigma)$ with $\mu \in \QQ,\ \sigma \in \QQ_+$ may encode the function $[\![\gamma]\!] : \RR \rightarrow [0,\infty)$ given as $[\![\gamma]\!](x) = \frac{1}{\sigma\sqrt{2\pi}} \exp{- \frac{(x - \mu)^2}{2\sigma^2}}$.
Without restricting ourselves to any particular encoding, we assume that $\Gamma$ is a \emph{profile language}, i.e., a finitely presented but usually infinite set of valid profiles. For any $\Gamma_0 \subseteq \Gamma$ we write $[\![\Gamma_0]\!] = \{[\![\gamma]\!] \mid \gamma \in \Gamma_0\}$.

We use profiles to encode HMMs $C = (Q, \Sigma, \Psi)$:
we say that $C$ is \emph{over}~$\Gamma$ if the observation density matrix~$\Psi$ is given as a matrix of pairs $(p_{i,j}, \gamma_{i,j}) \in \QQ_+  \times \Gamma$ such that $\Psi_{i,j} = p_{i,j} [\![\gamma_{i,j}]\!]$ and $\int_{\Sigma} [\![\gamma_{i,j}]\!]\,d\lambda = 1$ hold for all $i,j \in [Q]$. In this way the $p_{i,j}$ form the transition probabilities of the embedded Markov chain and the $\gamma_{i,j}$ encode the probability densities of the observations upon each transition.

\begin{example} \label{ex-encoding-prelims}
For a suitable profile language~$\Gamma$, the HMM from \cref{ex-HMMdef} may be over~$\Gamma$, with the observation density matrix given as
\begin{equation}
\begin{pmatrix}
(\frac12, (\Exp,2)) && (\frac12, (U,-1,0)) \\
(\frac13, (U,0,2))  && (\frac23, (\Exp,1))
\end{pmatrix}\,. \tag*{\qed}
\end{equation}
\end{example}
The observation density matrix~$\Psi$ of a HMM $(Q, \Sigma, \Psi)$ with \emph{finite}~$\Sigma$ can be given as a
list of matrices $\Psi(a) \in \QQ_+^{|Q| \times |Q|}$ for all $a \in \Sigma$ such that $\sum_{a \in \Sigma} \Psi(a)$ is a stochastic matrix.

\section{Likelihood Exponents}

Let $\pi_1$ and $\pi_2$ be initial distributions we define the likelihood ratio $L_n$ as a random variable on $\Sigma^n$ given by $L_n(w) = \frac{\pi_1 \Psi(w) \1^T}{\pi_2 \Psi(w) \1^T}$. Sometimes the notation $L_n^{\pi_1, \pi_2}$ is used when the initial distributions are not clear.

\begin{theorem}
The random variable $\frac1n \ln L_n$ converges $\pi_2$-a.s. to a finite number of values in $[-\infty, 0]$
\end{theorem}

We call these possible limits \emph{Likelihood Exponents} and a proof is given in \Cref{seccompdet}.

\begin{lemma}
Suppose that $\PP_{\pi_1}(L_n < 1) - \PP_{\pi_2}(L_n < 1) \geq 1 - e^{An + B}$ for some $A, B \in \RR$.
\end{lemma}

\begin{proof}
Fix $A < \alpha < 0$ and define the event $W_n = \{1 > L_n \geq \e^{n\alpha}\}$. Then
\begin{align*}
\PP_{\pi_2}(\lim_{n \rightarrow \infty} \frac1n \ln L_n > \alpha) & \leq \PP_{\pi_2}(\liminf_n \{\frac1n \ln L_n \geq \alpha\}) \\
& \leq \liminf_n \PP_{\pi_2}(\frac1n \ln L_n \geq \alpha) \\
& \leq \liminf_n \PP_{\pi_2}(L_n \geq \e^{n\alpha}) \\
& = \liminf_n \Big[ \PP_{\pi_2}(1 > L_n \geq \e^{n\alpha}) +  \PP_{\pi_2}(L_n \geq 1) \Big]\\
& \leq \liminf_n \Big[ \sum_{w \in W_n} \pi_2 \Psi(w) \1^T + e^{An + B} \Big]\\
& \leq \liminf_n \Big[ e^{-n\alpha} \sum_{w \in W_n} \pi_1 \Psi(w) \1^T\Big]\\
& \leq \liminf_n \Big[ e^{-n\alpha} \PP_{\pi_1}(\L_n < 1)\Big]\\
& \leq \liminf_n \Big[ e^{-n\alpha} e^{An + B}\Big]\\
& = 0.
\end{align*}
\end{proof}
Now suppose $\pi_1$ and $\pi_2$ are distinguishable.  By Theorem 5 of \cite{kief16} one may compute a $c \in (-\infty, 0)$ such that 
\begin{align*}
\PP_{\pi_1}(L_n < 1) - \PP_{\pi_2}(L_n < 1) & \geq 1 - 2e^{cn} \\
& \geq 1 - 2e^{\Lambda n}
\end{align*}

For a given HMM $(Q, \Sigma, \Psi)$, we may define a \emph{monitor} as a function $M_n : \Sigma^n \rightarrow \{1,2\}$. A well designed monitor reads an input word from an HMM started with either $\pi_1$ or $\pi_2$ and aims to return $1$ or $2$ respectively with high probability. However the following series of inequalities hold

\begin{align*}
\PP_{\pi_2}(M_n(w) = 2) - \PP_{\pi_1}(M_n(w) = 2)  &= \PP_{\pi_2}(M_n(w) = 1) + \PP_{\pi_1}(M_n(w) = 2) \\
& = \sum_{w \in \Sigma^n} \pi_1 \Psi(w) \1^T \delta_{M_n(w) = 1} + \pi_2 \Psi(w) \1^T \delta_{M_n(w) = 2} \\
& \geq \sum_{w \in \Sigma^n} \pi_1\Psi(w)\1^T \land \pi_2 \Psi(w) \1^T \\
& = \PP_{\pi_2}(L_n \leq 1) - \PP_{\pi_1}(L_n \leq 1) \\
& \geq 1 - 2e^{\Lambda n}
\end{align*}
which means for any monitor, to guarantee an error probability bound of at most $\epsilon$, we require atleast $\frac{\ln(\epsilon) - \ln 2}{\Lambda}$ observations. This bound motivates us to investigate computability properties of $\Lambda$.

\section{Computing Likelihood Exponents}\label{seccompdet}

We first discuss the convergence properties of $\liexp$. We say a set of states in an HMM $S \subseteq Q$ is \emph{irreducible} if for all $i,j \in S$, there exists a word $w \in \Sigma^*$ such that $\Psi(w)_{i,j} > 0$. An HMM is irreducible if $Q$ is irreducible. An irreducible HMM is \emph{mortal} if for some word $w \in \Sigma^*$, $\Psi(w) = 0$. 

Consider a discrete probability measure $r : \Sigma \rightarrow [0,1]$. We may construct a single state HMM $(q, \Sigma, \Psi_{\text{ind}})$ that produces a sequence of independent letters sampled from the distribution given by $r$ by letting $\Psi_{\text{ind}}(a) = r(a)$.

Now consider the union of this HMM with an irreducible HMM $(Q, \Sigma, \Psi)$ with the same set of observations. The union has states $Q \cup \{q\}$, observations $\Sigma$ and observation density matrix defined in blocks as 
\begin{equation}
\overline{\Psi}(a)  = \begin{pmatrix}
\Psi(a) && 0\\
0  && \Psi_{\text{ind}}(a)\\
\end{pmatrix}.
\end{equation}
For arbitrary initial distribution $\pi_1$ and $\pi_2 = \delta_q$ fixed, convergence of $\liexp$ is a consequence of \cite{prot13}. We write $\PPind$ for the probability measure on $\Sigma^\omega$ corresponding to words produced starting from $\pi_2$. A version of the main theorem is stated below.

\begin{theorem}\textsc{\textbf{Protasov's Theorem}}
Let $\pi_1$ be an initial distribution. If an irreducible HMM $(Q, \Sigma, \Psi)$ is mortal, then $\lyapexp = -\infty \quad \PPind$-a.s. otherwise there is a $\lambda \in (-\infty, 0]$ such that $\lyapexp = \lambda \quad \PPind$-a.s.
\end{theorem}
Since, $\liexp = \lyapexp - \lim_{n \rightarrow \infty} \frac1n \ln \pi_2 \Psi_n \1^T$, in this case convergence of $\liexp$ follows from Protasov's Theorem. When the HMM is not irreducible, we may generalise this theorem as follows

In the next section we show how to reduce the general case to this one.

\subsection{Cross Product Construction}

When the probability space on infinite words is derived from a general HMM, the probability of a particular letter being produced at a specific position in the infinite word depends on the state that the HMM is in. To overcome this issue, we build a \emph{cross-product cocycle} where current state of the producing HMM as started from $\pi_2$ is incorporated into the state space of the HMM started from $\pi_1$. 

To accomplish this, we simulate transitions in the producing HMM by sampling a uniform random number in the interval $[0,1)$. At each state, we may partition $[0,1)$ so that each sub-interval corresponds to specific transition and the size of the sub-interval corresponds to the probability of said transition. The union over all states of these partitions has a minimal finite $\sigma$-algebra. The atoms of this $\sigma$-algebra are also a partition of $[0,1)$ and so we may sample them independently at random with probabilities according to their size. The transformation is demonstrated in the diagram below.

\begin{center}
	\begin{tikzpicture}[scale=2.3,LMC style]
	\node[state] (s0) at (-1,1) {$s_0$};
	\node[state] (s1) at (-1,0) {$s_1$};
	\node[state] (s2) at (1,1) {$s_0$};
	\node[state] (s3) at (1,0) {$s_1$};

	
	\path[->] (s0) edge [loop,out=70,in=110,looseness=10] node[pos=0.5,above] {$\frac13 a$} (s0);
	\path[->] (s1) edge [loop,out=250,in=290,looseness=10] node[pos=0.5,below] {$\frac12 a$} (s1);
	\path[->] (s1) edge[bend left] node[left,pos=0.5] {$\frac12 b$} (s0);
	\path[->] (s0) edge[bend left] node[right,pos=0.5] {$\frac23 b$} (s1);
	
	
	\path[->] (s2) edge [loop,out=70,in=110,looseness=10] node[pos=0.5,above] {$\frac13 [0,\frac13)$} (s2);
	\path[->] (s3) edge [loop,out=190,in=230,looseness=10] node[pos=0.5,below] {$\frac13 [0,\frac13)$} (s3);
	\path[->] (s3) edge [loop,out=310,in=350,looseness=10] node[pos=0.5,below] {$\frac16 [\frac13,\frac12)$} (s3);
	
	\path[->] (s3) edge[bend left] node[left,pos=0.5] {$\frac12 [\frac12, 1)$} (s2);
	\path[->] (s2) edge[bend left] node[right,pos=0.5] {$\frac16 [\frac13,\frac12)$} (s3);
	\path[->] (s2) edge[bend left, out=80, in=100, looseness = 2.5] node[right,pos=0.5] {$\frac12 [\frac12, 1)$} (s3);
	
	\end{tikzpicture}
\end{center}

Consider the HMM $(Q, \Sigma, \Psi)$ with finite alphabet $\Sigma$. Since for each $i \in [Q]$, $\sum_{a \in \Sigma} \sum_{j = 1}^{|Q|} \Psi(a)_{i,j} = 1$ it follows that we may define a function $\rho_i : [0,1) \rightarrow Q \times \Sigma$ such that for all $i \in [Q]$, $\MLeb(\rho_i^{-1}\{(j, a)\}) = \Psi(a)_{i,j}$. Consider the minimal $\sigma$-algebra $\sigma\{\rho_i^{-1}\{(j, a)\} \mid i,j \in [Q], a \in \Sigma\}$ which is finite and has a set of atomic elements $P$ of at most $|Q|^2|\Sigma|$ elements. $P$ is also a partition of $[0,1)$. Let $p \in P$ then $\rho_i(x)$ is constant for all $x \in p$ so we may overload the notation and consider the function $\rho_i : P \rightarrow Q \times \Sigma$. 

We will describe the one-state chain $(\{1\}, P, \Psi_P)$ as the \emph{singleton generator} for $\Psi$ where $\Psi_P(p) = \MLeb(p)$. Given an initial distribution $\pi$ for $(Q, \Sigma, \Psi)$, a word generated by its singleton generator uniquely defines a path of states and letters. Let $\PPind$ be the measure on the set of infinite words $P^\omega$ generated by this HMM.

In order to incorporate the state space of the producing HMM into the state space of the chain started from $\pi_1$, we define two functions $l : Q \times \Sigma \rightarrow Q$ and $r : Q \times \Sigma \rightarrow \Sigma$ where $l(q,a) = q$ and $r(q,a) = a$. We then define the cross-product cocycle $\Psi^* : P \rightarrow [0,1]^{(Q \times Q) \times (Q \times Q)}$ as 
\[\Psi^*(p)_{(i_1,j_1),(i_2,j_2)} = \begin{cases} 
\Psi(r \circ \rho_{j_1}(p) )_{i_1, i_2} & l \circ \rho_{j_1}(p) = j_2 \\
0 & \text{else}. \\
\end{cases}\]

We may extend $\Psi^*$ in the usual way to $P^n$ and write $\Psi_n^*$ for the associated random variable on $P^n$ under measure $\PPind$. This leads to the following Theorem.

\begin{theorem}
Consider the HMM $(Q, \Sigma, \Psi)$ with initial distributions $\pi_1$ and $\pi_2$ then for any measurable set $A \in [-\infty, 0]$
\begin{equation*}
\PP_{\pi_2}\big( \lim_{n\rightarrow\infty} \frac1n \ln L_n \in A\big) = \PPind\big(\lim_{n\rightarrow\infty} \frac1n \ln \frac{\pi_1 \times \pi_2 \Psi^* \1}{\pi_2 \times \pi_2 \Psi^* \1} \in A \big).
\end{equation*}
In addition, the random variable $\lim_{n\rightarrow\infty} \frac1n \ln L_n$ takes at most $|Q|^2$ values with non-zero probability under the $\PPind$ measure.
\end{theorem}

We will write $\Epsilon_{\pi_1, \pi_2}$ for the set of values attained by $\liexp$ with $\PPind$ non-zero probability. 

\begin{corollary}\label{subcompcalc}
Let $(Q, \Sigma, \Psi)$ then we may produce $K \leq |Q|^2$ irreducible cocycles $\Psi_k$ and pairs of initial distributions $(a_1, b_1) \dots, (a_K, b_K)$ in time $O(|Q|^4|\Sigma|)$ such that $\lim_{n \rightarrow \infty} \frac1n \ln L_n^{a_k, b_k}$ converges to a constant $\PP_{b_k}$-almost surely and
\begin{equation*}
\Epsilon_{\pi_1, \pi_2} \subseteq \bigcup_{k = 1}^K \Epsilon_{a_k, b_k}.
\end{equation*}
\end{corollary}

\section{Deterministic Chains}
A HMM $(Q, \Sigma, \Psi)$ is deterministic if for all $a \in \Sigma$, all rows of $\Psi(a)$ contain exactly one non-zero entry. If $q$ is a starting state, for any word in $w \in \Sigma$ the vector $\delta_q \Psi(w)$ contains at most one non-zero entry. A simple example of a deterministic chain is the following example.

\begin{example}\label{twostateex}
Let $1 > \theta > \phi \geq \frac12$ and consider the disconnected HMM

\begin{center}
	\begin{tikzpicture}[scale=2.3,LMC style]
	\node[state] (s0) at (-1,0) {$s_0$};
	\node[state] (s1) at (1,0) {$s_1$};
	
	\path[->] (s0) edge [loop,out=200,in=160,looseness=10] node[pos=0.5,left] {$\theta a$} (s0);
	\path[->] (s0) edge [loop,out=20,in=340,looseness=10] node[pos=0.5,right] {$(1 - \theta) b$} (s0);
	\path[->] (s1) edge [loop,out=200,in=160,looseness=10] node[pos=0.5,left] {$\phi a$} (s1);
	\path[->] (s1) edge [loop,out=20,in=340,looseness=10] node[pos=0.5,right] {$(1 - \phi) b$} (s1);
	\end{tikzpicture}.
\end{center}
Starting the chain from $s_1$ leads to an independent production of letters. Additionally in this case, the expressions $e_{s_0}\Psi(w)\1^T$ can be written as $\theta^k (1 - \theta)^{n - k}$ where $k$ is the number of observations labelled $a$ in $w$. Similarly, the likelihood ratio $L_n(w) = \Big(\frac{\theta}{\phi}\Big)^k \Big(\frac{1 - \theta}{1 - \phi}\Big)^{n-k}$ and so 
\begin{equation*}
\frac1n \ln L_n = \frac{k}{n}\ln\frac{\theta}{\phi} + \frac{n - k}{n}\ln\frac{1 - \theta}{1- \phi} \rightarrow \phi \ln\frac{\theta}{\phi} + (1 - \phi) \ln\frac{1 - \theta}{1- \phi} 
\end{equation*}
as $n \rightarrow \infty$ since the ratio $k/n$ tends to $\phi$ almost surely. Therefore the likelihood exponent is symbolically computable. 
\end{example}
More generally the following holds in the case of deterministic chains.

\begin{proposition}\label{deterministiccomp}
Let $(Q, \Sigma, \Psi)$ be a deterministic HMM and let $p,q \in Q$ be initial states, then we may compute a set of numbers $\{\lambda_1, \dots, \lambda_K \}$ where $k \leq |Q|$ and a set of probabilities $\{ p_1, \dots, p_K \}$ such that
\begin{equation*}
\PP_{\delta_q} \Big( \lim_{n \rightarrow \infty} \frac1n \ln L_n = \lambda_k \Big) = p_k  
\end{equation*}
in time polynomial in $|Q|$ and $|\Sigma|$.
\end{proposition}

There are two considerations when designing such a polynomial time algorithm. The computation of the likelihood exponents comes from \Cref{subcompcalc}. The computation of the probabilities comes from computing hitting probabilities in a Markov chain. The following lemma will support this argument.

\begin{lemma}\label{determcrossprod}
Let the cocycle $\Psi$ be deterministic, then $\Psi^*$ is deterministic.
\end{lemma}

\begin{proof}
Fix $i_1, j_1 \in Q$ and consider the transition $\Psi^*(p)_{(i_1,j_1),(i_2,j_2)}$. There is at most one letter $a \in \Sigma$ that such that $\Psi_{i_1, i_2}(a)$ and $\Psi_{j_1, j_2}(a)$ are non-zero. By definition of $\Psi^*$ it must therefore be deterministic.
\end{proof}

\begin{lemma}\label{computablelyaps}
Let $(Q, \Sigma, \Psi)$ be a deterministic, irreducible cocycle then we may compute a symbollic number $\lambda \in [-\infty, 0]$ in time polynomial in $|Q|$ such that for any two starting state $q$ and independent producer of letters $\PP_{\text{ind}}$
\begin{equation*}
\PP_{\text{ind}} \Big( \lim_{n \rightarrow \infty} \frac1n \ln (e_q \Psi_n \1^T) = \lambda \Big) = 1.
\end{equation*}
\end{lemma}

\begin{proof}
By Protasov's theorem, such a $\lambda$ exists. In the case that for some $a \in \Sigma$ there is a zero row in $\Psi(a)$, $\lambda = -\infty$ since the irreducibility property guarantees we hit every state infinitely often. In the case that for all $a \in \Sigma$, all rows in $\Psi(a)$ contain at least one non-zero element we may conclude that $\lambda > -\infty$ since every word is producible from starting state $q$. This condition can be checked in $O(|Q|^2)$ time. In the case that $\lambda > -\infty$ consider the random sequence $i_1, \dots, i_n$ defined by $\Psi_{i_{k - 1}, i_k}(a_k) > 0$ for $k \geq 2$ and $i_1 = q$. Then write $r_k = \Psi_{i_{k - 1}, i_k}(a_k)$. The sequences $i_1, \dots, i_n$ and $r_1, \dots, r_n$ are well defined because $\Psi$ is deterministic and for all $a \in \Sigma$ there are no non-zero rows. Since $\Psi_\text{min} \leq \frac1n \ln (e_q \Psi_n \1^T) \leq \Psi_\text{max}$ by the dominated convergence theorem,
\begin{align*}
\lim_{n \rightarrow \infty} \frac1n \ln (\delta_q \Psi_n \1^T) & = \lim_{n \rightarrow \infty} \EE_{\text{ind}} [ \frac1n \ln (e_q \Psi_n \1^T) ] \\
& =  \lim_{n \rightarrow \infty} \EE_{\text{ind}} [ \frac1n \ln \big( r_1(a_1) \dots r_n(a_n)\big) ] \\
& =  \lim_{n \rightarrow \infty} \EE_{\text{ind}} [ \frac1n \sum_{k = 1}^n \ln r_k(a_k) ] \\
& =  \lim_{n \rightarrow \infty}  \frac1n \sum_{k = 1}^n \EE_{\text{ind}} [ \ln r_k(a_k) ]. \\
\end{align*}
The random variable $\ln r_k(a_k)$ depends only on $i_k$ and $a_k$. We may construct a Markov chain $M_{i,j}$ on $Q$ by taking $M_{i,j} = \sum_{a \in \Sigma} \delta_{\Psi(a)_{i,j} > 0} \PP(a)$ which is possible in $O(|Q|^2 |\Sigma|)$ time. Assume $M$ has period $p$ then writing $k = sp + r$, $e_q M^{sp + r}$ converges to the vector $\mu_r$ as $p \rightarrow \infty$, and $i_k$ has a distribution given by $e_q M^n$. It follows that

\begin{align*}
\lim_{n \rightarrow \infty} \frac{1}{n}\sum_{k = 1}^n \EE_{\text{ind}}[\ln r_k(a_k)] &= \lim_{s \rightarrow \infty} \frac{1}{sp}\sum_{s = 0}^n \sum_{r = 1}^p \EE_{\text{ind}}[\ln r_{sp + r}(a_{sp + r})] \\
& = \frac{1}{p}  \sum_{r = 1}^p \lim_{s \rightarrow \infty} \frac{1}{s}\sum_{s = 0}^n \EE_{\text{ind}} [\ln r_{sp + r}(a_{sp + r})] \\
& =  \frac{1}{p}  \sum_{r = 1}^p \sum_{i = 1}^{|Q|} \sum_{a \in \Sigma} (\mu_r)_i \ln \Psi(a)_{i,j} .\\
\end{align*}  
The vector $\mu_r$ can be computed as the stationary distribution of $M^r$ in time $O(|Q|^3)$. Hence the entire sum can be computed in $O(|Q|^4|\Sigma|)$.
\end{proof}


\begin{lemma}\label{deterministicdistributionproblem}
Let $\Psi : \Sigma \rightarrow [0,1]^{|Q| \times |Q|}$ be a deterministic cocycle over $\Sigma$ and let $i \in Q$ be an initial state. One can determine in polynomial time a set of lyapunov exponents $\{\lambda_1, \dots, \lambda_K \}$ where $K \leq |Q|$ and for each $k \leq K$ the probabilities $\PP_{\text{ind}}(\lim_{n \rightarrow \infty} \frac{1}{n} \ln (\|\e_i \Psi^n\|) = \lambda_k)$. 
\end{lemma}

\begin{proof}(sketch)
Let $P_1, \dots, P_K$ be the bottom connected components of the graph of $\Psi$. The restriction $\Psi$ to any end component $P_k$ is an irreducible cocycle. Thus, by \Cref{computablelyaps} there are lyapunov exponents $\lambda_k$ computable in polynomial time such that conditioned on hitting $P_k$ $\lim_{n \rightarrow \infty} \frac{1}{n} \ln (\|\e_i \Psi^n\|) = \lambda_k$ almost surely. It remains to compute these hitting probabilities from state $i$ which can be done in $O(|Q|^3)$ time.
\end{proof}

\begin{proof}[proof of \Cref{deterministiccomp}]
By \Cref{determcrossprod} the cocycle $\Psi^*$ is deterministic. Furthermore,  
\begin{equation}\label{liexpequality}
\liexp = \lim_{n \rightarrow \infty} \frac1n \ln \frac{\delta_p \times \delta_q \Psi^*_n \1^T}{\delta_q \times \delta_q \Psi^*_n \1^T} = \lim_{n \rightarrow \infty} \frac1n \ln \delta_p \times \delta_q \Psi^*_n \1^T - \lim_{n \rightarrow \infty} \frac1n \ln \delta_q \times \delta_q \Psi^*_n \1^T.
\end{equation}
Since the vector $\delta_p \times \delta_q \Psi^*(w)$ contains at most one non-zero entry, the irreducible end component hit by $\delta_p \times \delta_q \Psi^*(w)$ uniquely determines the end component hit by $\delta_q \times \delta_q \Psi^*(w)$. Therefore by \Cref{deterministicdistributionproblem} we may compute the hitting probabilities of the irreducible end components as started from $\delta_p \times \delta_q$ and in turn by \Cref{liexpequality} gives us the distribution over the limits of $\liexp$.
\end{proof}
\section{Non-deterministic Chains}

---- SECTION ABOUT APPROXIMATING THE LIKELIHOOD EXPONENT IN THE CASE OF IRREDUCIBLE CHAINS ----

In the case of non-deterministic chains, a word from the producing chain may lead to support on more than one irreducible component. For example, starting the chain below from $s_0$ with state $s_3$ producing the letters leads to a situation where the support of $\delta_{s_0}\Psi_n$ has support on $s_1$ and $s_2$ $\PP_{\delta_{s_3}}$-almost surely. 

\begin{center}
	\begin{tikzpicture}[scale=2.3,LMC style]
	\node[state] (s0) at (-1,0) {$s_0$};
	\node[state] (s1) at (-0.5,0.5) {$s_1$};
	\node[state] (s2) at (-0.5,-0.5) {$s_2$};
	
	\node[state] (s3) at (0.5,0) {$s_3$};
	\node[state] (s4) at (1.25,0) {$s_4$};
	
	\path[->] (s1) edge [loop,out=200,in=160,looseness=10] node[pos=0.5,left] {$\frac13 a$} (s1);
	\path[->] (s1) edge [loop,out=20,in=340,looseness=10] node[pos=0.5,right] {$\frac23 b$} (s1);
	\path[->] (s2) edge [loop,out=200,in=160,looseness=10] node[pos=0.5,left] {$\frac14 a$} (s2);
	\path[->] (s2) edge [loop,out=20,in=340,looseness=10] node[pos=0.5,right] {$\frac34 b$} (s2);
	
	\path[->] (s0) edge [out=45,in=225] node[pos=0.5,left] {$\frac12 a$} (s1);
	\path[->] (s0) edge [out=315,in=135] node[pos=0.5,left] {$\frac12 a$} (s2);
	
	
	
	\path[->] (s3) edge [out=0,in=180] node[pos=0.5,below] {$a$} (s4);
	\path[->] (s4) edge [loop,out=290,in=250,looseness=10] node[pos=0.5,below] {$\frac12 a$} (s4);
	\path[->] (s4) edge [loop,out=110,in=70,looseness=10] node[pos=0.5,above] {$\frac12 b$} (s4);
	\end{tikzpicture}.
\end{center}

In this case, for $w \in \Sigma^n$ and $k$ being the number of $a$ observations in $w$, 

\begin{equation*}
L_n(w) = \Big(\frac{\frac13}{\frac12}\Big)^k \Big(\frac{\frac23}{\frac12}\Big)^{n-k} + \Big(\frac{\frac14}{\frac12}\Big)^k \Big(\frac{\frac34}{\frac12}\Big)^{n-k} = \Big(\frac23\Big)^k \Big(\frac43\Big)^{n-k} + \Big(\frac12\Big)^k \Big(\frac32\Big)^{n-k}.
\end{equation*}

\Cref{distributedofsumconv} tells us how to calculate the $\liexp$ in this case - the maximum of the two likelihood exponents obtained starting the chain from $s_1$ and $s_2$ respectively.

\begin{lemma}\label{distributedofsumconv}
Let $(a_n^k)_{n = 1}^\infty$ for $k = 1, \dots, K$ be sequences in $(0,1]$ such that $\frac1n \ln a_n^k$ converges. Then, 
\begin{equation*}
\lim_{n \rightarrow \infty}\frac1n \ln \Big( \sum_{k = 1}^K a_n^k \Big) = \max_{k} \Big\{\lim_{n \rightarrow \infty}\frac1n \ln a_n^k \Big\}.
\end{equation*}
\end{lemma}









\begin{proposition}\textsc{\textbf{NFA language inclusion}}
NFA Language inclusion is PSPACE-complete.
\end{proposition}

\begin{corollary}
Let $(Q_1, \Sigma, \Psi_1)$ and $(Q_2, \Sigma, \Psi_2)$ be HMMs where the latter is connected and gives rise to probability measure $\PP_2$ on $\Sigma^\omega$. Assume both HMMs are represented as arrays of binary fractions.

Deciding whether $\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| \Psi_1(w) \| = -\infty$ $\PP_2$-a.s. is PSPACE-complete.
\end{corollary}

\begin{proof}
Suppose that for all $n \in \NN$ and $w \in \Sigma^n$ such that $\PP_2(w) > 0$ there exists $i,j \in [Q]$ such that $\Psi(w)_{i,j} > 0$. Then $\Psi(w)_{i,j} \geq \min_{i,j,a} \Psi(a)_{i,j}^n$ and hence $\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| \Psi(w) \| \geq \ln (\min_{i,j,a} \Psi(a)_{i,j})$. Since the limit is $\PP_2$-a.s. by \cref{kingmancor} it follows that $\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| \Psi_1(w) \| = -\infty$ if and only if there is some word $w \in \Sigma^*$ such that $\Psi_1(w) = 0$ and $\PP_2(w) > 0$. 

Consider an instance of the Mortality problem on $\{0,1\}$ matrices given by a matrix valued function $M : \Sigma \rightarrow  \{0,1\}^{N \times N}$. If $\sum_{a \in \Sigma, k \in [N]} M(a)_{i,k} = 0$ for some $i \in [N]$ then $M$ is mortal if and only if the sub-problem $M'$ is mortal where $M'$ is the matrix valued function $M$ but with the $i$th row and column removed. Therefore we may assume WLOG that $\sum_{a \in \Sigma, k \in [N]} M(a)_{i,k} > 0$ for all $i \in [N]$.  

Let $(\Psi_1(a))_{i,j} = M(a)_{i,j} / \sum_{a \in \Sigma, k \in [N]} M(a)_{i,k}$ and let $\Psi_2(a) = \frac{1}{|\Sigma|}$. Then $([N], \Sigma, \Psi_1)$ and $(\{1\}, \Sigma, \Psi_2)$ are valid HMMs and $\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| \Psi_1(w) \| = -\infty$ $\PP_2$-a.s. if and only if $M$ is mortal.

Now consider two HMMs $(Q_1, \Sigma, \Psi_1)$ and $(Q_2, \Sigma, \Psi_2)$ where the latter is connected. The transition function of an NFA can be represented as a matrix valued function $\Delta_i : \Sigma \rightarrow \{0,1\}^{|Q_i| \times |Q_i|}$. So consider the NFAs (for $i = 1, 2$) $N_i = (Q_i, \Sigma, \supp~ \Psi_i, q_i, Q)$ where $q_2$ is picked arbitrarily from $Q_2$. It follows that $\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| \Psi_1(w) \| = -\infty$ $\PP_2$-a.s. if and only if $\LL(M_1) \subseteq \LL(M_2)$ for all choices of $q_1 \in Q_1$.
\end{proof}

\subsubsection{Mortality Problem for 0-1 matrices}
Consider a cocycle with 0-1 entries $\Psi : \Sigma \rightarrow \{0,1\}^{N \times N}$. Decide whether there exists a word $w \in \Sigma^*$ such that $\Psi(w) = 0$.

\begin{theorem}
The Mortality problem for 0-1 matrices is PSPACE-complete.
\end{theorem}

The proof of this theorem is due to \cite{karasha09}. The problem is PSPACE-hard even when the cocycle forms an HMM by the following corollary

\begin{corollary}
The Mortality problem for HMMs is PSPACE-complete.
\end{corollary}

\begin{proof}
To show PSPACE-hardness, consider an instance of the mortality problem for a 0-1 matrix $M : \Sigma \rightarrow  \{0,1\}^{N \times N}$. If $\sum_{a \in \Sigma, k \in [N]} M(a)_{i,k} = 0$ for some $i \in [N]$ then $M$ is mortal if and only if the sub-problem $M'$ is mortal where $M'$ is the matrix valued function $M$ but with the $i$th row and column removed. Therefore we may assume WLOG that $\sum_{a \in \Sigma, k \in [N]} M(a)_{i,k} > 0$ for all $i \in [N]$. Let $\Psi(a)_{i,j} = M(a)_{i,j} / \sum_{a \in \Sigma, k \in [N]} M(a)_{i,k}$. Then $([N], \Sigma, \Psi_1)$ is a valid HMM and is mortal if and only if $M$ is mortal.

To show the problem is PSPACE-complete, given an instance of the mortality problem for an HMM $\Psi$, we construct an instance of the mortality problem for 0-1 cocycles simply by considering $\supp ~\Psi$.
\end{proof}

\subsection{Lyapunov distribution problem}
Consider an HMM $\Psi^* : \Sigma \rightarrow [0,1]^{N \times N}$ and a probability distribution $\PP_{\text{ind}}$ over $\Sigma^\omega$ where the letters are i.d.d. Let $P_1, \dots, P_K \subset [N]$ be the irreducible components of $\Psi^*$. By Protasov's theorem, upon restriction to a single irreducible component, $\frac{1}{n}\ln \| \Psi^* \restriction_{P_k} \|$ converges almost surely. Therefore there exists an assignment from irreducible end components to Lyapunov exponents $\rho : [K] \rightarrow \{\lambda_1, \dots, \lambda_n \}$.

Given an initial state $i$ compute a probability distribution $\mu$ over $[K]$ such that $\PP_{\text{ind}}^*(\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| e_i \Psi^*_n\| = \rho(k)) = \sum_{l : \rho(l) = \rho(k)} \mu(l)$.

\begin{theorem}
The Lyapunov distribution problem is PSPACE-hard. 
\end{theorem}

\begin{proof}
Suppose we have an algorithm to compute the probabilities in the Lyapunov distribution problem. Consider an instance of the Mortality problem with cocycle $\Psi : \Sigma \rightarrow [0,1]^{N \times N}$. Write $\Sigma = \{a_1, \dots, a_K \}$. We may compute $\Psi_{\text{min}}$ the smallest non zero value of all the matrices in $\Psi$ in time $O(K N^2)$. Consider uniformly picked letters in $\Sigma$ and a function $f : \Sigma \rightarrow [0,1]$, 

\begin{equation*}
f(a) = \begin{cases}
(\Psi_{\text{min}} / 2)^K & a = a_1 \\
(1 - (\Psi_{\text{min}} / 2)^K) / (K - 1) & a \neq a_1. \\	
\end{cases}
\end{equation*}
We construct a new cocycle $\overline{\Psi} : \Sigma \rightarrow [0,1]^{(N + 2) \times (N + 2)}$ built in blocks:

\begin{equation*}
	\overline{\Psi}(a) = \begin{pmatrix}
	0 & 1/2|\Sigma| & \1/2N|\Sigma|\\
	0 & f(a) & 0\\
	0 & 0 & \Psi(a)
	\end{pmatrix}
\end{equation*}
where $\1/2|\Sigma|$ is a row vector with $N$ elements which are identically $1/2|\Sigma|$. The second state is deterministic and also an end component so the following inequality due to \Cref{computablelyaps}  holds when $i = 2$

\begin{equation}\label{lam1ineq}
\lambda_1 = \lim_{n \rightarrow \infty} \frac{1}{n} \ln (\| e_2 \overline{\Psi} \|) = \ln (\Psi_{\text{min}} / 2) + \frac{K - 1}{K} \ln \frac{1 - (\Psi_{\text{min}} / 2)^K}{(K - 1) } < \ln (\Psi_{\text{min}} / 2).
\end{equation}

Now consider the case $i = 1$ and suppose $\Psi$ has Lyapunov exponent $\lambda_2$ with respect to uniformly chosen letters. Then, 
\begin{align*}
\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| e_i \overline{\Psi}\| &= \lim_{n \rightarrow \infty} \frac{1}{n} \ln \big( f^n + \| \Psi\| \big) \\
& = \max_{j = 1, 2}\{\lambda_1, \lambda_2\} \quad \text{a.s}.
\end{align*}
\Cref{lam1ineq} guarantees that $\lambda_1 \neq \lambda_2$. We may run our algorithm for the Lyapunov distribution problem on $\Psi^*$ with $i = 1$ which yields a distribution $p$ and $1 - p$ on the symbols $\lambda_1$ and $\lambda_2$ respectively such that $p \in \{0,1\}$. If $\Psi$ is not mortal then $\lambda_1 < \ln(\Psi_{\text{min}} / 2) < \ln \Psi_{\text{min}} < \lambda_2$ hence $p = 0$. If $\Psi$ is mortal then $-\infty = \lambda_2 < \lambda_1$ hence $p = 1$.
\end{proof}


\subsection{Lyapunov comparison problem}
Consider two irreducible cocycles $\Psi_1, \Psi_2 : \Sigma \rightarrow [0,1]^{N \times N}$ with lyapunov exponents $\lambda_1, \lambda_2$ respectively. Decide whether $\lambda_1 < \lambda_2$.

\subsubsection{Problem 1}
Consider a cocycle $\Psi^* : \Sigma \rightarrow \RR^{N \times N}$ and a probability distribution $\PP_{\text{ind}}$ over $\Sigma^\omega$ where the letters are i.d.d. Let $P_1, \dots, P_K \subset [N]$ be the irreducible components of $\Psi^*$ ordered in such a way that $\lambda : [K] \rightarrow [-\infty, 0)$ defined as $\lambda(k) = \lim_{n \rightarrow \infty} \frac{1}{n} \ln \|\Psi^*_n \restriction_{P_k}\|$ is monotonically increasing with respect to $k$. Given an initial state $i$ compute a probability distribution $\mu$ over $[K]$ such that $\PP_{\text{ind}}^*(\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| e_i \Psi^*_n\| = \lambda(k)) = \mu(k)$.

\subsubsection{Problem 2}
Consider two NFAs $M_i = (Q, \Sigma, \Delta, q_0, F_i)$ for $i = 1, 2$ where $F_1$ and $F_2$ are disjoint and probability measure $\PP_{\text{ind}}$ on $\Sigma^\omega$ where the letters are i.i.d. Compute $\PP_{\text{ind}}(\mathcal{L}(M_1) \setminus \mathcal{L}(M_2))$.

\subsubsection{Problem 3}
Consider two DFAs $M_i = (Q, \Sigma, \delta, q_0, F_i)$ for $i = 1, 2$ where $F_1$ and $F_2$ are disjoint and probability measure $\PP_{\text{ind}}$ on $\Sigma^\omega$ where the letters are i.i.d. Compute $\PP_{\text{ind}}(\mathcal{L}(M_1) \setminus \mathcal{L}(M_2))$.

\subsubsection{Problem 3*}
Consider a DFA $D = (Q, \Sigma, \delta, q_0, F)$ and probability measure $\PP_{\text{ind}}$ on $\Sigma^\omega$ where the letters are i.i.d. Compute $\PP_{\text{ind}}(\mathcal{L}(D))$.

\subsubsection{Problem 4}
Let $(Q, M)$ be a Markov chain and let $F \subset Q$ be set of states. Given an initial state $i$, compute the hitting probability of $F$.


\begin{lemma}
Given an instance of problem 1 we can compute in logarithmic space at most N instances of problem 2 such that if we can solve problem 2 in PSPACE we can solve problem 1 in PSPACE.
\end{lemma}

\begin{proof}
Consider an instance of Problem 1. Let $Q = [N]$, $q \in \Delta(p, a) \iff \Psi^*(a)_{p,q} > 0$, $q_0 = i$. For $k = 1, \dots, K - 1$ we set $F_k^- = \bigcup_{j = 1}^k P_j$ and $F_k^+ = \bigcup_{j = k + 1}^K P_j$. After running our algorithm for Problem 2 on the NFAs $M_k^1 = (Q, \Sigma, \Delta, q_0, F_k^+)$ and $M_k^2 = (Q, \Sigma, \Delta, q_0, F_k^-)$ for each $k = 1, \dots, K - 1$, we obtain the probabilities $p_k = \PP_{\text{ind}}(\mathcal{L}(M_k^1) \setminus \mathcal{L}(M_k^2))$. Write $\mu_1 = p_1$ then compute $\mu_k = p_{k + 1} - p_k$ for each $k$. This gives the required probability distribution.
\end{proof}

\begin{lemma}
An instance of problem 3 can be reduced to an instance of problem 4 in time $O(|\Sigma| |Q|^2)$. 
\end{lemma}

\begin{proof}
Consider an instance of Problem 3. Recall that $\delta : Q \times \Sigma \rightarrow Q$. We construct an instance of Problem 4 where $Q$ and $F$ remain the same, $i = q_0$ and assigning the entries of $M$ by $M_{i,j} = \sum_{a \in \Sigma} \PP_{\text{ind}}(a) \chi_{\delta(i,a) = j}$. Computing the hitting probability of $F$ in the Markov chain gives $\PP_{\text{ind}}(\mathcal{L}(D))$ since the paths in the Markov chain hitting $F$ correspond to cylinder sets of words whose union is $\mathcal{L}(D)$.
\end{proof}

\begin{lemma}
Problem 4 can be computed in polynomial time.
\end{lemma}




\bibliography{lyapunovhmm}

\appendix





\begin{center}
	\begin{tikzpicture}[scale=2.3,LMC style]
	\node[state] (s0s0) at (0,0) {$s_0$,$s_0$};
	\node[state] (s0s1) at (-1,0) {$s_0$,$s_1$};
	\node[state, fill=red] (s0s2) at (-1,-1) {$s_0$,$s_2$};
	\node[state] (s1s0) at (1,0) {$s_1$,$s_0$};
	\node[state, fill=green] (s1s1) at (0,1) {$s_1$,$s_1$};
	\node[state, fill=red] (s1s2) at (1,1) {$s_1$,$s_2$};
	\node[state, fill=red] (s2s0) at (1,-1) {$s_2$,$s_0$};
	\node[state, fill=red] (s2s1) at (-1,1) {$s_2$,$s_1$};
	\node[state] (s2s2) at (0,-1) {$s_2$,$s_2$};

	\path[->] (s0s1) edge [loop,out=200,in=160,looseness=10] node[pos=0.3,left] {$\frac13 [0,1)$} (s0s1);
	\path[->] (s1s0) edge [loop,out=20,in=340,looseness=10] node[pos=0.3,right] {$1 [0,\frac13)$} (s1s0);
	\path[->] (s2s2) edge [loop,out=250,in=290,looseness=10] node[pos=0.35,below] {$\frac12 [0, \frac12)$} (s2s2);
	\path[->] (s0s0) edge [loop,out=305,in=345,looseness=10] node[pos=0.4,right] {$\frac13 [0,\frac13)$} (s0s0);
	\path[->] (s1s1) edge [loop,out=70,in=110,looseness=10] node[pos=0.4,above] {$1 [0,1)$} (s1s1);
	
	\path[->] (s0s0) edge node[above,pos=0.4] {$\frac13 [\frac13, \frac23)$} (s0s1);
	\path[->] (s0s0) edge node[right,pos=0.8] {$\frac13 [\frac13, \frac23)$} (s2s1);
	\path[->] (s0s0) edge node[right,pos=0.6] {$\frac13 [\frac13, \frac23)$} (s1s1);
	\path[->] (s0s0) edge node[above,pos=0.6] {$\frac13 [0, \frac13)$} (s1s0);
	\path[->] (s0s0) edge node[left,pos=0.9] {$\frac13 [\frac23, 1)$} (s1s2);
	\path[->] (s0s0) edge node[left,pos=0.4] {$\frac13 [\frac23, 1)$} (s0s2);
	
	\path[->] (s0s1) edge node[left,pos=0.9] {$\frac13 [0, 1)$} (s2s1);
	\path[->] (s2s2) edge node[below,pos=0.4] {$\frac12 [0, \frac12)$} (s0s2);
	\path[->] (s1s0) edge node[right,pos=0.8] {$1 [\frac23, 1)$} (s1s2);
	\path[->] (s2s2) edge node[below,pos=0.6] {$\frac12 [\frac12, 1)$} (s2s0);
	
	\path[->] (s0s0) edge[out=290, in=70] node[right,pos=0.6] {$\frac13 [\frac23, 1)$} (s2s2);
	\path[->] (s2s2) edge[out=110, in=250] node[left,pos=0.2] {$\frac12 [\frac12, 1)$} (s0s0);
	
	\path[->] (s0s1) edge[loop, out=135, in=135, looseness=2.6] node[left,pos=0.4] {$\frac13 [0, 1)$} (s1s1);
	\path[->] (s1s0) edge[loop, out=45, in=45, looseness=2.6] node[right,pos=0.4] {$1 [\frac13, \frac23)$} (s1s1);
	\end{tikzpicture}
\end{center}

\begin{proof}
then we can extend $\rho_i$ to $\rho_i : P^n \rightarrow (Q \times \Sigma)^n$ by iteratively defining $\rho_i(ua) = \rho_i(u) \rho_{l(\rho_i(u))}(a)$. 

$\sum_{p \in P}\Psi^*(p)$ is stochastic so defines an HMM $(Q \times Q, P, \Psi^*)$ with probability measure $\PP_{\text{ind}}$ and we may extend to $\Psi^* : P^n \rightarrow [0,1]^{(Q \times Q) \times (Q \times Q)}$ in the usual way. Let  $i_0, j_0 \in [Q]$ be initial states, It follows that for a word $u_1 \dots u_n \in \{\rho_{j_0}(u_1 \dots u_n) = (j_1, a_1), \dots, (j_n, a_n)\}$,

\begin{align*}
\| e_{i_0}^T e_{j_0} \Psi^*(u) \| & = \sum_{(i_1, \dots, i_n) \in [Q]^n} \Psi^*(u_1)_{(i_0, j_0),(i_1,j_1)} \dots \Psi^*(u_n)_{(i_{n - 1}, j_{n - 1}),(i_n,j_n)}\\
& = \sum_{(i_1, \dots, i_n) \in [Q]^n} \Psi(a_1)_{i_0, i_1} \dots \Psi(a_n)_{i_{n - 1}, i_n}\\
& = \| e_{i_0} \Psi(a_1 \dots a_n) \|.
\end{align*}
It follows that for any initial distributions $\pi_1, \pi_2$, $\| \pi_1^T \pi_2 \Psi^*(u) \| = \| \pi_1 \Psi(a_1 \dots a_n) \|$. Then considering the word $a_1, \dots, a_n$, 

\begin{align*}
\PP_{\pi_2}(a_1 \dots a_n) & =\|\pi_2\Psi(a_1 \dots a_n)\| \\
& = \sum_{j_1, \dots, j_n \in [Q]^n}\|\pi_2\Psi(a_1)_{\pi_2, j_1} \dots \Psi(a_n)_{j_{n - 1}, j_n}\|\\
& = \sum_{u_1, \dots, u_n \in \{\rho_{\pi_2}(u_1, \dots, u_n) = (j_1, a_1), \dots, (j_n, a_n)\}} \Psi_P(u_1 \dots u_n)\\
& = \PP_{\text{ind}}(l \circ \rho_{\pi_2}(u_1 \dots u_n) = a_1 \dots a_n)
\end{align*}
Hence for any $f : [0,1] \rightarrow [0,1]$
\begin{equation*}
\int_{\Sigma^n} f(\|\pi_1 \Psi \|) d\PP_{\pi_2} = \int_{P^n} f(\| \Psi^* \|) d\PP_{\text{ind}}.
\end{equation*}
\end{proof}


Consider the bottom connected components $C_1, \dots, C_k \subseteq Q \times Q$ of the Markov chain defined by $\sum_{p \in P} \Psi^*(p)$. 

Let $d : Q \times Q \rightarrow Q \times Q$ be defined as $d(s_1, s_2) = (s_2, s_2)$.

\begin{lemma}\label{Qboundfordenominator}
Let $s$ be a starting state for an HMM $(Q, \Sigma, \Psi)$. Then $\lim_{n \rightarrow \infty} \frac1n \ln \|  (s, s) \Psi_n^* \|$ takes at most $|Q|$ values.
\end{lemma}

\begin{proof}
Let $P_1, \dots, P_K$ be the irreducible components of $\Psi^*$. Consider states $(s_1, s_2), (r_1, r_2) \in Q \times Q$. If there is a path from $(s_1, s_2)$ to $(r_1, r_2)$ then there is also a path from $(s_2, s_2)$ to $(r_2, r_2)$. Therefore for any end component $P_i$ it follows that the image $d(P_i) \subseteq P_j$ for some end component $P_j$ and so we may define a function $\rho : \{ P_1, \dots, P_K \} \rightarrow \{ P_1, \dots, P_K \}$ such that $\rho(P_i) = P_j$. Suppose $P_i, P_j$ have Lyapunov exponents $\lambda_i$ and $\lambda_j$ respectively. Let $\pi_1$ and $\pi_2$ be initial distributions such that the support of $\pi_1$ is in $P_i$ and the support of $\pi_2$ is in $P_j$ then the likelihood ratio $L_n = \frac{\| \pi_1 \Psi_n \|}{\| \pi_2 \Psi_n \|}$ converges to a limit in the set $[0,\infty)$ with respect to the measure $\PP_{\pi_2}$, the same limit as $\frac{\| (\pi_1, \pi_2) \Psi_n^* \|}{\| (\pi_2, \pi_2) \Psi_n^* \|}$ with respect to $\PP_\text{ind}$. Since both $\frac{1}{n} \ln \| (\pi_1, \pi_2) \Psi_n^* \|$ and $\frac{1}{n} \ln \| (\pi_2, \pi_2) \Psi_n^* \|$ converge almost surely in the set $[-\infty, 0]$ to $\lambda_i$ and $\lambda_j$ respectively, $\frac{1}{n} \ln \frac{\| \pi_1 \Psi_n \|}{\| \pi_2 \Psi_n \|}$ converges in $[-\infty, 0]$ with respect to $\PP_{\pi_2}$. Therefore $\lambda_i \leq \lambda_j$.
Now consider $\lim_{n \rightarrow \infty} \frac1n \ln \|  (s, s) \Psi_n^* \|$ whose possible limits is bounded by $|Q|^2$. Suppose for some word $w \in P^n$ the support of $(s, s) \Psi_n^*$ intersects an irreducible component $P_i$. Then it must also intersect $P_j$. Since $\lambda_i \leq \lambda_j$ it follows that $\lim_{n \rightarrow \infty} \frac1n \ln \|  (s, s) \Psi_n^* \| \geq \lambda_j$. Since the image $\rho \{P_1, \dots, P_K \} \leq |Q|$ it follows that $\lim_{n \rightarrow \infty} \frac1n \ln \|  (s, s) \Psi_n^* \|$ takes at most $|Q|$ values.
\end{proof}

\begin{lemma}
Consider an HMM $(Q, \Sigma, \Psi)$. Let $\Epsilon : [0,1]^{|Q|} \times [0,1]^{|Q|} \rightarrow [\infty, 0]$ be a parametrised random variable on $\Sigma^\omega$ defined by $\Epsilon(\pi_1, \pi_2) = \lim_{n \rightarrow \infty} \frac1n \ln \frac{\| \pi_1 \Psi(w) \|}{\|\pi_2 \Psi(w) \|}$. Then $|\{\Epsilon(\pi_1, \pi_2) \mid \pi_1, \pi_2 \in [0,1]^{|Q|}\} | \leq |Q|^2$ with respect to the measure $\PP_{\pi_2}$.
\end{lemma}

\begin{proof}
First consider the case of dirac distributions $\pi_1 = \delta_r$ and $\pi_2 = \delta_s$. We may instead consider a bound on
\begin{equation*}
\lim_{n \rightarrow \infty} \frac{1}{n} \ln \frac{\| (\delta_r, \delta_s) \Psi_n^* \|}{\| (\delta_s, \delta_s) \Psi_n^* \|} = \lim_{n \rightarrow \infty} \frac1n \ln \| (\delta_r, \delta_s) \Psi_n^*\| - \lim_{n \rightarrow \infty} \frac1n \ln \| (\delta_s, \delta_s) \Psi_n^*\|
\end{equation*}
with respect to the $\PP_\text{ind}$ measure. Let $C_1, \dots, C_K$ be the irreducible lethal components of $\Psi^*$. For $L \leq K$ without loss of generality suppose $C_1, \dots, C_L$ be the irreducible components that are also end components containing diagonal entries. Let $R_1, \dots, R_L \subseteq Q$ be disjoint and have the property that for all $q \in R_i$, $(q, q) \in C_i$. 

Given a state $q_i \in Q$ any letter in $P$ defines a unique subsequent state $q_j$ and a unique letter produced $a$. Therefore, projecting $(\delta_p, \delta_q) \Psi^*_n$ onto its right component, yields a point distribution on some state. Therefore the function $\zeta : \{(\delta_p, \delta_q) \Psi^*(w) \mid w \in P^*\} \rightarrow Q$ defined by $\zeta((\delta_p, \delta_q) \Psi^*(w)) = r$ if and only if $\supp \sum_{i = 1}^{|Q|} ((\delta_p)_i, \delta_q) \Psi^*(w))_{i,j} = (\delta_r)_j$ for all $j$ is well defined.

We may partition $\Sigma^\omega$ into $W_1, \dots, W_L$ such that $\zeta((\delta_r, \delta_s) \Psi^*_n(w))$ hits all states in $R_k$ infinitely often for $w \in W_k$. It follows that $(\delta_s, \delta_s) \Psi^*_n(w)$ intersects the end component $C_k$ and hits no other end components with diagonal entries. let $q \in C_k$ then $\frac1n \ln(\delta_s, \delta_s) \Psi^*_n(w)$ must converge almost surely on $W_k$ to the Lyapunov exponent given by $\lim_{n \rightarrow \infty} \frac1n \ln(\delta_q, \delta_q) \Psi^*_n(w)$.

Similarly $\zeta((\delta_r, \delta_s) \Psi^*_n(w)) \in R_k$ and so $(\delta_r, \delta_s) \Psi^*_n(w)$ is contained in the set $Q \times R_k$ for $w \in W_k$. Since $\zeta((\delta_r, \delta_s) \Psi^*_n(w))$ hits all states in $R_k$ infinitely often each irreducible component $P_i$ such that $P_i \leq (\delta_r, \delta_s) \Psi^*_n(w)$ must have the property that $|P_i| \geq |R_k|$. Therefore the total number of irreducible components hit by $(\delta_r, \delta_s) \Psi^*_n(w)$ where $w \in W_k$ is at most $|Q|$. Since $L \leq |Q|$ the total number of possible limits for 

\begin{equation*}
\lim_{n \rightarrow \infty} \frac{1}{n} \ln \frac{\| (\delta_r, \delta_s) \Psi_n^* \|}{\| (\delta_s, \delta_s) \Psi_n^* \|}
\end{equation*}
is $|Q|^2$. It remains to show that $\{\Epsilon(\pi_1, \pi_2) \mid \pi_1, \pi_2 \in [0,1]^{|Q|}\} \subseteq \{\Epsilon(\delta_r, \delta_s) \mid r, s \in Q\}$. Fix $\pi_1$ and $\pi_2$ and let us consider the possible values of 
\begin{equation*}
\lim_{n \rightarrow \infty} \frac1n \ln \| (\pi_1, \delta_s) \Psi_n^*\| - \lim_{n \rightarrow \infty} \frac1n \ln \| (\pi_2, \delta_s) \Psi_n^*\|
\end{equation*}
where $s \in \supp ~\pi_2$. A consider again the partition of $\Sigma^\omega = \bigcup_{k = 1}^L W_k$. For $w \in W_k$, the only end component in $C_1, \dots, C_L$ hit by $(\pi_2, \delta_s) \Psi^*_n(w)$ is $C_k$. It follows that $\lim_{n \rightarrow \infty} \frac1n \ln \| (\pi_2, \delta_s) \Psi_n^* \| = \lim_{n \rightarrow \infty} \frac1n \ln \| (\delta_s, \delta_s) \Psi_n^* \|$. Any strongly connected component hit by $(\pi_1, \delta_s) \Psi_n^*(w)$, is also hit by $(\delta_r, \delta_s) \Psi_n^*(w)$ for some $r \in ~\supp \pi_1$.  It follows that we may partition $\Sigma^\omega$ so that on each part of the partition there is some $s, r \in Q$ such that 
\begin{equation*}
\lim_{n \rightarrow \infty} \frac1n \ln \frac{ \| (\pi_1, \delta_s) \Psi_n^*\| }{ \| (\pi_2, \delta_s) \Psi_n^*\|} = \lim_{n \rightarrow \infty} \frac1n \ln \frac{ \| (\delta_r, \delta_s) \Psi_n^*\| }{ \| (\delta_s, \delta_s) \Psi_n^*\|}.
\end{equation*}
\end{proof}

\begin{lemma}
let $\alpha \in (0,1)$ then
\begin{equation*}
{n \choose \alpha n } \simeq \Big( \alpha^\alpha (1 - \alpha)^{1 - \alpha} \sqrt{2\pi n \alpha (1 - \alpha)} \Big)^{-n}
\end{equation*}
\end{lemma}

\begin{example}
Let $1 > \theta > \phi \geq \frac12$ and consider the disconnected HMM

\begin{center}
	\begin{tikzpicture}[scale=2.3,LMC style]
	\node[state] (s0) at (-1,0) {$s_0$};
	\node[state] (s1) at (1,0) {$s_1$};
	
	\path[->] (s0) edge [loop,out=200,in=160,looseness=10] node[pos=0.5,left] {$\theta a$} (s0);
	\path[->] (s0) edge [loop,out=20,in=340,looseness=10] node[pos=0.5,right] {$(1 - \theta) b$} (s0);
	\path[->] (s1) edge [loop,out=200,in=160,looseness=10] node[pos=0.5,left] {$\phi a$} (s1);
	\path[->] (s1) edge [loop,out=20,in=340,looseness=10] node[pos=0.5,right] {$(1 - \phi) b$} (s1);
	\end{tikzpicture}
\end{center}

By (INSERT THM) we may compute $\lim_{n \rightarrow \infty} \frac{1}{n} \ln L_n$ to be
\begin{equation*}
\phi \ln \theta + (1 - \phi) \ln (1 - \theta))  - \phi \ln \phi -  (1 - \phi) \ln (1 - \phi))
\end{equation*}
Suppose $w \in \Sigma^n$ contains $k$ instances of the letter $a$, then
$\PP_{s_0} (w) = \theta^{k} (1 - \theta)^{n - k}$ and $\PP_{s_1} (w) = \phi^{k} (1 - \phi)^{n - k}$ it follows that

\begin{align*}
\PP_{s_0}(w) \leq \PP_{s_1}(w) & \iff \theta^{k} (1 - \theta)^{n - k} \leq \phi^{k} (1 - \phi)^{n - k} \\
& \iff \Big( \frac{\theta (1 - \phi)}{\phi (1 - \theta)} \Big)^k \leq \Big( \frac{1 - \phi}{1 - \theta}\Big)^n \\
& \iff k \leq n ~\frac{\ln \frac{1 - \phi}{1 - \theta}}{\ln\frac{\theta(1 - \phi)}{\phi(1 - \theta)}} \\
\end{align*}

Let $\gamma = \frac{\ln \frac{1 - \phi}{1 - \theta}}{\ln\frac{\theta(1 - \phi)}{\phi(1 - \theta)}}$, then using the inequalities $1 - \frac{1}{x} \leq \ln x \leq x - 1$ it follows that 

\begin{equation*}
\gamma  = \frac{1}{1 + \ln \frac{\theta}{\phi} / \ln \frac{1 - \phi}{1 - \theta}} \leq \frac{1}{1 + (1 - \frac{\phi}{\theta})/ (\frac{1 - \phi}{1 - \theta} - 1)} = \frac{\frac{1 - \phi}{1 - \theta} -  1}{\frac{1 - \phi}{1 - \theta} - \frac{\phi}{\theta}} = \theta \frac{\theta - \phi}{\theta - \phi \theta - \phi + \phi\theta} = \theta.
\end{equation*}
Similarly $\phi \leq \gamma$.

The function $s \rightarrow \theta^s(1 - \theta)^{n - s}$ which by considering its derivative is increasing for $0 \leq s \leq \theta n$. Similarly, $s \rightarrow \phi^s(1 - \phi)^{n - s}$ is decreasing for $\phi n \leq s \leq n$.


\begin{align*}
\lim_{n \rightarrow \infty} \Big(\sum_{w \in \Sigma^n} \text{min}(w) \Big)^\frac{1}{n} & = \lim_{n \rightarrow \infty} \Big( \sum_{k = 0}^{\floor*{\gamma n}} {n \choose k} \theta^k(1 - \theta)^{n - k} + \sum_{k = \floor*{\gamma n}+1}^{n} {n \choose k} \phi^k(1 - \phi)^{n - k} \Big)^{\frac{1}{n}} \\
& = \frac{ \max \{ \theta^\gamma (1 - \theta)^{1- \gamma}, \phi^\gamma (1 - \phi)^{1- \gamma}\}}{\gamma^\gamma (1 - \gamma)^{1 - \gamma}}
\end{align*}




\end{example}

\end{document}
  