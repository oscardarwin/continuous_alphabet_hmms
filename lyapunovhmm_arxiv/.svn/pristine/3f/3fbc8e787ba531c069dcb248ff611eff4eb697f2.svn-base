\documentclass[a4paper,UKenglish,cleveref, autoref,mathscr]{lipics-v2019}
%This is a template for producing LIPIcs articles.
%See lipics-manual.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
%for section-numbered lemmas etc., use "numberwithinsect"
%for enabling cleveref support, use "cleveref"
%for enabling cleveref support, use "autoref"

\usepackage{bbm, tikz, mathtools, thm-restate}
\usetikzlibrary{arrows,calc,automata,intersections}
\tikzset{LMC style/.style={>=angle 60,every edge/.append style={thick},every state/.style={thick,minimum size=20,inner sep=0.5}}}

\usepackage{asymptote}

\newcommand{\eow}{\$}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\Epsilon}{\mathcal{E}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\e}{\mathrm{e}}
\newcommand{\Bor}{\mathscr{B}}
\newcommand{\BorC}{\mathscr{C}}
\newcommand{\GG}{\mathscr{G}}
\newcommand{\HH}{\mathscr{H}}
\newcommand{\Norm}{\mathscr{N}}
%\newcommand{\FF}{\mathscr{F}}
\newcommand{\DD}{\mathscr{D}}
\newcommand{\LL}{\mathscr{L}}
\newcommand{\Exp}{\mathit{Exp}}
\newcommand{\MM}{\mathscr{M}}
\newcommand{\Basis}{\mathscr{B}}
\newcommand{\balph}{\boldsymbol{\alpha}}
\newcommand{\bpldP}{\boldsymbol{P}}
\newcommand{\stoch}{\boldsymbol{S}}
\newcommand{\TT}{\boldsymbol{T}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\pem}{\mathbf{A}}
\newcommand{\con}{\textbf{con}}
\newcommand{\Con}{\overline{\textbf{con}}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\pl}{\Gamma_{\mathit{GEM}}}
\newcommand{\MLeb}{\MM_{\mathit{Leb}}}

\graphicspath{ {images/} }

\DeclareMathOperator{\Span}{span\,}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

%\newcommand{\stefan}[1]{\marginpar{\textcolor{blue}{#1}}}

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Equivalence of Hidden Markov Models with Continuous Observations}

\author{Oscar Darwin}{Department of Computer Science, Oxford University, United Kingdom }{}{https://orcid.org/0000-0001-5016-014X}{}%TODO mandatory, please use full name; only 1 author per \author macro; first two parameters are mandatory, other parameters can be empty. Please provide at least the name of the affiliation and the country. The full address is optional

\author{Stefan Kiefer}{Department of Computer Science, Oxford University, United Kingdom}{}{https://orcid.org/0000-0003-4173-6877}{}

\authorrunning{O. Darwin and S. Kiefer}%TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{John Q. Public and Joan R. Public}%TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

%\ccsdesc[100]{General and reference~General literature}
%\ccsdesc[100]{General and reference}%TODO mandatory: Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm
\ccsdesc[500]{Theory of computation~Random walks and Markov chains}
\ccsdesc[500]{Mathematics of computing~Stochastic processes}
\ccsdesc[300]{Theory of computation~Logic and verification}

\keywords{Markov chains, equivalence, probabilistic systems, verification}%TODO mandatory; please add comma-separated list of keywords

\category{}%optional, e.g. invited paper

\relatedversion{}%optional, e.g. full version hosted on arXiv, HAL, or other respository/website
%\relatedversion{A full version of the paper is available at \url{...}.}

\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...

%\funding{(Optional) general funding statement \dots}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

%\acknowledgements{I want to thank \dots}%optional

%\nolinenumbers %uncomment to disable line numbering

%\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\sloppy
\begin{document}

\maketitle

\begin{abstract}
Lyapunov
\end{abstract}

\section{Introduction}


\section{Preliminaries}
We write $\NN$ for the set of positive integers, $\QQ$ for the set of rationals and $\QQ_+$ for the set of positive rationals.
For $d \in \NN$ and a finite set $Q$ we use the notation $|Q|$ for the number of elements in $Q$, $[d] = \{1, \dots, d\}$ and $[Q] = \{1, \dots, |Q|\}$. Vectors $\mu \in \RR^N$ are viewed as row vectors and we write $\1 = (1, \dots, 1) \in \RR^N$.
Superscript~$T$ denotes transpose; e.g., $\1^T$ is a column vector of ones.
A matrix $M \in \RR^{N \times N}$ is \emph{stochastic} if $M$ is non-negative and $\sum_{j = 1}^{N} M_{i,j} = 1$ for all $i \in [N]$.
For a domain $\Sigma$ and subset $E \subset \Sigma$ the \emph{characteristic} function $\chi_E : \Sigma \rightarrow \{0,1\}$ is defined as $\chi_E(x) = 1$ if $x \in E$ and $\chi_E(x) = 0$ otherwise. Given a function $\gamma : [N] \rightarrow S \subseteq [N]$ and a matrix $M \in \RR^{N \times N}$ clearly $S$ is isomorphic to $[M]$ for some $M \leq N$ by a unique monotonically increasing isomorphism $\phi : S \rightarrow [M]$. We may define the restricted matrix $(M\restriction_S)_{i,j} = M_{\gamma^{-1 } \circ \phi^{-1} (i), \gamma^{-1 } \circ \phi^{-1} (j)}$.

Throughout this paper, we use $\Sigma$ to denote a set of \emph{observations}.
We assume $\Sigma$ is a topological space and $(\Sigma, \GG, \lambda)$ is a measure space where every open subset $E \in \GG$ has non-zero measure. Indeed $\RR$ and the usual Lebesgue measure space on $\RR$ satisfy these assumptions.
The set $\Sigma^n$ is the set of words over~$\Sigma$ of length $n$ and $\Sigma^* = \bigcup_{n = 0}^\infty \Sigma^n$.


A matrix valued function $\Psi : \Sigma \rightarrow [0,\infty)^{N \times N}$ can be integrated element-wise.
We write $\int_E \Psi\,d\lambda$ for the matrix with entries $\left( \int_E \Psi\, d\lambda \right)_{i,j} = \int_E \Psi_{i,j}\, d\lambda$, where $\Psi_{i,j} : \Sigma \rightarrow [0,\infty)$ is defined by $\Psi_{i,j}(x) = \big( \Psi(x) \big)_{i,j}$ for all $x \in \Sigma$.


A function $f : \Sigma \rightarrow \RR^m$ is \emph{piecewise continuous} if there is an open set $C \subset \Sigma$, called a \emph{set of continuity}, such that $f$ is continuous on $C$ and for every point $x \in  \Sigma \setminus C$ there is some sequence of points $x_n \in C$ such that $\lim_{n \rightarrow \infty} x_n = x$ and $\lim_{n \rightarrow \infty} f(x_n) = f(x)$. For a non-negative function $f : \Sigma \rightarrow [0,\infty)$ we use the notation ${\supp~f = \{x \in \Sigma \mid f(x) > 0\}}$.


\begin{definition}\label{HMMdef}
A \emph{Hidden Markov Model} (HMM) is a triple $(Q, \Sigma, \Psi)$ where $Q$ is a finite set of states, $\Sigma$ is a set of observations, and the \emph{observation density matrix} $\Psi : \Sigma \rightarrow [0,\infty)^{|Q| \times |Q|}$ specifies the transitions such that $\int_\Sigma \Psi\, d\lambda$ is a stochastic matrix.
\end{definition}
\begin{example} \label{ex-HMMdef}
The HMM from the introduction is the triple $(\{q_1, q_2\}, \mathbb{R}, \Psi)$ with
\begin{equation}
\Psi(x) \ = \ \begin{pmatrix}
\frac12 \cdot 2 \exp(-2 x) \cdot \chi_{[0,\infty)}(x) && \frac12 \cdot 1 \cdot \chi_{[-1,0)}(x) \\
\frac13 \cdot \frac12 \cdot \chi_{[0,2)}(x) && \frac23 \cdot \exp(-x) \cdot \chi_{[0,\infty)}(x)
\end{pmatrix}\,. \tag*{\qed}
\end{equation}
\end{example}
We assume that $\Psi$ is piecewise continuous and extend $\Psi$ to the mapping $\Psi : \Sigma^* \rightarrow [0,\infty)^{|Q| \times |Q|}$ with $\Psi(x_1 \cdots x_n) = \Psi(x_1) \times \dots \times \Psi(x_n)$ for $x_1, \dots, x_n \in \Sigma$. If $C$ is the set of continuity for $\Psi : \Sigma \rightarrow [0,\infty)^{|Q| \times |Q|}$, then for fixed $n \in \NN$ the restriction $\Psi : \Sigma^n \rightarrow [0,\infty)^{|Q| \times |Q|}$ is piecewise continuous with set of continuity $C^n$. We say that $A \subseteq \Sigma^n$ is a \emph{cylinder set} if $A = A_1 \times \dots \times A_n$ and $A_i \in \GG$ for $i \in [n]$. For every $n$ there is an induced measure space $(\Sigma^n, \GG^n, \lambda^n)$ where $\GG^n$ is the smallest $\sigma$-algebra containing all cylinder sets in~$\Sigma^n$ and $\lambda^n(A_1 \times \dots \times A_n) = \prod_{i = 1}^n \lambda(A_i)$ for any cylinder set $A_1 \times \dots \times A_n$. Let $A \subset \Sigma^n$ and write $A \Sigma^\omega$ for the set of infinite words over~$\Sigma$ where the first $n$ observations fall in the set~$A$. Given a HMM $(Q, \Sigma, \Psi)$ and initial distribution $\pi$ on $Q$ viewed as vector $\pi \in \RR^{|Q|}$, there is an induced probability space $(\Sigma^\omega, \GG^*, \PP_\pi)$ where $\Sigma^\omega$ is the set of infinite words over~$\Sigma$, and $\GG^*$ is the smallest $\sigma$-algebra containing (for all $n \in \NN$) all sets $A \Sigma^\omega$ where $A\subseteq \Sigma^n$ is a cylinder set and $\PP_\pi$ is the unique probability measure such that
$\PP_\pi(A \Sigma^\omega) =  \pi \int_A \Psi\, d\lambda^n \1^T$
for any cylinder set $A \subseteq \Sigma^n$.
\begin{definition}
For two distributions $\pi_1$ and $\pi_2$ and a HMM $C = (Q, \Sigma, \Psi)$, we say that $\pi_1$ and~$\pi_2$ are \emph{equivalent}, written $\pi_1 \equiv_C \pi_2$, if $\PP_{\pi_1}(A) = \PP_{\pi_2}(A)$ holds for all measurable subsets $A \subseteq \Sigma^\omega$.
\end{definition}
One could define equivalence of two pairs $(C_1,\pi_1)$ and $(C_2,\pi_2)$ where $C_i = (Q_i, \Sigma, \Psi_i)$ are HMMs and $\pi_i$ are initial distributions for $i=1,2$.
We do not need that though, as we can define, in a natural way, a single HMM over the disjoint union of $Q_1$ and~$Q_2$ and consider instead equivalence of $\pi_1$ and~$\pi_2$ (where $\pi_1,\pi_2$ are appropriately padded with zeros).

%A key concept used throughout this paper is that of \emph{functional decomposition}.
Given an observation density matrix $\Psi$, a \emph{functional decomposition} consists of functions $f_k : \Sigma \rightarrow [0,\infty)$ and matrices $P_k \in \RR^{|Q| \times |Q|}$ for $k \in [d]$ such that $\Psi(x) = \sum_{k = 1}^d f_k(x) P_k$ for all $x \in \Sigma$ and $\int_{\Sigma} f_k\, d\lambda = 1$ for all $k \in [d]$. We sometimes abbreviate this decomposition as $\Psi = \sum_{k = 1}^d f_k P_k$ and this notion has a central role in our paper.

\begin{example} \label{ex-functional-decomposition}
The observation density matrix~$\Psi$ from \cref{ex-HMMdef} has a functional decomposition
\begin{align*}
\Psi(x) \ = \
& 2 \exp(-2 x) \chi_{[0,\infty)}(x)
\begin{pmatrix}
\frac12 && 0 \\ 0 && 0
\end{pmatrix}
+
\chi_{[-1,0)}(x)
\begin{pmatrix}
0 && \frac12 \\ 0 && 0
\end{pmatrix}
+ \mbox{} \\
& \frac12 \chi_{[0,2)}(x)
\begin{pmatrix}
0 && 0 \\
\frac13 && 0
\end{pmatrix}
+
\exp(-x) \chi_{[0,\infty)}(x)
\begin{pmatrix}
0 && 0 \\
0 && \frac23
\end{pmatrix}
\,. \tag*{\qed}
\end{align*}
\end{example}

\begin{lemma}\label{stochasticPk}
Let $(Q, \Sigma, \Psi)$ be a HMM.
If $\Psi$ has functional decomposition $\Psi =  \sum_{k = 1}^d f_k P_k$ then $\sum_{k = 1}^d P_k$ is stochastic.
\end{lemma}
\begin{proof}
By definition of a HMM, $\int_{\Sigma} \Psi\, d\lambda$ is stochastic, and we have
\begin{equation*}
\int_{\Sigma} \Psi\, d\lambda = \int_{\Sigma} \sum_{k = 1}^d f_k P_k\, d\lambda =  \sum_{k = 1}^d P_k  \int_{\Sigma} f_k\, d\lambda =  \sum_{k = 1}^d P_k.\qedhere
\end{equation*}
\end{proof}
When $\Sigma$ is finite, it follows that $\int_\Sigma \Psi\, d\lambda = \sum_{a \in \Sigma} \Psi(a)$. Hence $\sum_{a \in \Sigma} \Psi(a)$ is stochastic.

\subparagraph*{Encoding}
For computational purposes we assume that rational numbers are represented as ratios of integers in binary.
The initial distribution of a HMM with state set~$Q$ is given as a vector $\pi \in \QQ^{|Q|}$.
We also need to encode continuous functions, in particular, density functions such as Gaussian, exponential or piecewise-polynomial functions.
A \emph{profile} is a finite word (i.e., string) that describes a continuous function.
It may consist of (an encoding of) a function type and its parameters. For example, the profile  $(\mathcal{N}, \mu, \sigma)$ may denote a Gaussian (also called normal) distribution with mean $\mu \in \QQ$ and standard deviation $\sigma \in \QQ_+$. A profile may also consist of a description of a rational linear combination of such building blocks.
For any profile~$\gamma$ we write $[\![\gamma]\!] : \Sigma \rightarrow [0,\infty)$ for the function it encodes.
For example, a profile $\gamma = (\mathcal{N}, \mu, \sigma)$ with $\mu \in \QQ,\ \sigma \in \QQ_+$ may encode the function $[\![\gamma]\!] : \RR \rightarrow [0,\infty)$ given as $[\![\gamma]\!](x) = \frac{1}{\sigma\sqrt{2\pi}} \exp{- \frac{(x - \mu)^2}{2\sigma^2}}$.
Without restricting ourselves to any particular encoding, we assume that $\Gamma$ is a \emph{profile language}, i.e., a finitely presented but usually infinite set of valid profiles. For any $\Gamma_0 \subseteq \Gamma$ we write $[\![\Gamma_0]\!] = \{[\![\gamma]\!] \mid \gamma \in \Gamma_0\}$.

We use profiles to encode HMMs $C = (Q, \Sigma, \Psi)$:
we say that $C$ is \emph{over}~$\Gamma$ if the observation density matrix~$\Psi$ is given as a matrix of pairs $(p_{i,j}, \gamma_{i,j}) \in \QQ_+  \times \Gamma$ such that $\Psi_{i,j} = p_{i,j} [\![\gamma_{i,j}]\!]$ and $\int_{\Sigma} [\![\gamma_{i,j}]\!]\,d\lambda = 1$ hold for all $i,j \in [Q]$. In this way the $p_{i,j}$ form the transition probabilities of the embedded Markov chain and the $\gamma_{i,j}$ encode the probability densities of the observations upon each transition.

\begin{example} \label{ex-encoding-prelims}
For a suitable profile language~$\Gamma$, the HMM from \cref{ex-HMMdef} may be over~$\Gamma$, with the observation density matrix given as
\begin{equation}
\begin{pmatrix}
(\frac12, (\Exp,2)) && (\frac12, (U,-1,0)) \\
(\frac13, (U,0,2))  && (\frac23, (\Exp,1))
\end{pmatrix}\,. \tag*{\qed}
\end{equation}
\end{example}
The observation density matrix~$\Psi$ of a HMM $(Q, \Sigma, \Psi)$ with \emph{finite}~$\Sigma$ can be given as a
list of matrices $\Psi(a) \in \QQ_+^{|Q| \times |Q|}$ for all $a \in \Sigma$ such that $\sum_{a \in \Sigma} \Psi(a)$ is a stochastic matrix.

\section{Convergence to Lyapunov Exponent}

Let $(\Sigma^\omega, \GG^*, \mu)$ be a probability space on infinite words and let $M = (Q, \Sigma, \Psi)$ be an HMM. We define the equivalence relation on $[Q]$ as $i \sim_\mu j$ if and only if $\forall i, j \in [Q], v \in \Sigma^*$ there exists a word $w \in \Sigma^*$ such that $\mu(v w \Sigma^\omega) > 0$ and $\Psi(w)_{i,j} > 0$. $\sim_\mu$ defines a partition $P$ on the states of $Q$ and if $P = \{Q\}$ then we say $M$ is $\mu$-connected. Clearly if $\pi$ is an initial distribution for $M$ and $\PP_{\pi}$ the resulting measure on infinite words, then $M$ is ($\PP_\pi$-)connected if and only if $\int_\Sigma \Psi d\lambda$ is an irreducible stochastic matrix.

\begin{lemma}\label{absolutecontinuityprob1}
Let $(Q, \Sigma, \Psi)$ be a connected HMM and let $\pi$ be an initial distribution. Then $\int_\Sigma \Psi d\lambda$ has a stationary distribution $\mu$ with $\pi << \mu$ and therefore for any event $E \in \GG^*$, $\mu(E) = 1$ implies $\pi(E) = 1$.  
\end{lemma}

\begin{proof}
The stochastic matrix $\int_\Sigma \Psi d\lambda$ has a left eigenvector $\mu$ for the eigenvalue $1$ since $\int_\Sigma \Psi d\lambda \1^T = \1^T$. For any $i,j \in [Q]$ such that $\mu_i > 0$ by irredicibility and non-negativity, there is a $k \in \NN$ such that $(\int_\Sigma \Psi d\lambda)^k_{i,j} > 0$ and therefore $\mu_j = (\mu (\int_\Sigma \Psi d\lambda)^k)_j > 0$. Therefore, $\mu$ has full support and there is some $\alpha > 0$ such that $\pi \leq \alpha \mu$. Let $E \in \GG^*$ then there is a sequence of events $E_n \in \GG^n$ such that
\begin{align}
\pi(E) & = \lim_{n \rightarrow \infty} \pi(E_n) \\
& = \pi \big( \lim_{n \rightarrow \infty}\int_{E_n} \Psi d\lambda \big) \1^T \\
& \leq \alpha \mu \big( \lim_{n \rightarrow \infty}\int_{E_n} \Psi d\lambda \big) \1^T \\
& = \alpha \mu(E).
\end{align} 
Therefore it follows that $\pi << \mu$ and in particular $\mu(E) = 1 \implies \mu(E^c) = 0 \implies \pi(E^c) = 0 \implies \pi(E) = 1$.
\end{proof}

\subsection{Cross Product Chain}

Consider an HMM $(Q, \Sigma, \Psi)$ with finite alphabet $\Sigma$. Since for each $i \in [Q]$, $\sum_{a \in \Sigma} \sum_{j = 1}^{|Q|} \Psi(a)_{i,j} = 1$ it follows that we may define a function $\rho_i : [0,1] \rightarrow Q \times \Sigma$ such that for all $i \in [Q]$, $\MLeb(\rho_i^{-1}\{(j, a)\}) = \Psi(a)_{i,j}$. Consider the minimal $\sigma$-algebra $\sigma\{\rho_i^{-1}\{(j, a)\} \mid i,j \in [Q], a \in \Sigma\}$ which is finite and has an disjoint generating set $P$ of at most $|Q|^2|\Sigma|$ elements. Let $p \in P$ then $\rho_i(x)$ is constant for all $x \in p$ so we may overload the notation and consider the function $\rho_i : P \rightarrow Q \times \Sigma$. Let $l : Q \times \Sigma \rightarrow Q$ and $r : Q \times \Sigma \rightarrow \Sigma$ be given as $l(q,a) = q$ and $r(q,a) = a$ then we can extend $\rho_i$ to $\rho_i : P^n \rightarrow (Q \times \Sigma)^n$ by iteratively defining $\rho_i(ua) = \rho_i(u) \rho_{l(\rho_i(u))}(a)$.


We will describe the one-state chain $(\{1\}, P, \Psi_P)$ as the \emph{singleton generator} for $\Psi$ where $\Psi_P(p) = \MLeb(p)$. Given an initial distribution $\pi$ for $(Q, \Sigma, \Psi)$, a word generated by its singleton generator uniquely defines a path of states and letters. 

We define the cocycle $\Psi^* : P \rightarrow [0,1]^{(Q \times Q) \times (Q \times Q)}$ as 
\[\Psi^*(p)_{(i_1,j_1),(i_2,j_2)} = \begin{cases} 
\Psi(r \circ \rho_{j_1}(p) )_{i_1, i_2} & l \circ \rho_{j_1}(p) = j_2 \\
0 & \text{else}. \\
\end{cases}\]

$\sum_{p \in P}\Psi^*(p)$ is stochastic so defines an HMM $(Q \times Q, P, \Psi^*)$ with probability measure $\PP_{\text{ind}}$ and we may extend to $\Psi^* : P^n \rightarrow [0,1]^{(Q \times Q) \times (Q \times Q)}$ in the usual way. Let  $i_0, j_0 \in [Q]$ be initial states, It follows that for a word $u_1 \dots u_n \in \{\rho_{j_0}(u_1 \dots u_n) = (j_1, a_1), \dots, (j_n, a_n)\}$,

\begin{align*}
\| e_{i_0}^T e_{j_0} \Psi^*(u) \| & = \sum_{(i_1, \dots, i_n) \in [Q]^n} \Psi^*(u_1)_{(i_0, j_0),(i_1,j_1)} \dots \Psi^*(u_n)_{(i_{n - 1}, j_{n - 1}),(i_n,j_n)}\\
& = \sum_{(i_1, \dots, i_n) \in [Q]^n} \Psi(a_1)_{i_0, i_1} \dots \Psi(a_n)_{i_{n - 1}, i_n}\\
& = \| e_{i_0} \Psi(a_1 \dots a_n) \|.
\end{align*}
It follows that for any initial distributions $\pi_1, \pi_2$, $\| \pi_1^T \pi_2 \Psi^*(u) \| = \| \pi_1 \Psi(a_1 \dots a_n) \|$. Then considering the word $a_1, \dots, a_n$, 

\begin{align*}
\PP_{\pi_2}(a_1 \dots a_n) & =\|\pi_2\Psi(a_1 \dots a_n)\| \\
& = \sum_{j_1, \dots, j_n \in [Q]^n}\|\pi_2\Psi(a_1)_{\pi_2, j_1} \dots \Psi(a_n)_{j_{n - 1}, j_n}\|\\
& = \sum_{u_1, \dots, u_n \in \{\rho_{\pi_2}(u_1, \dots, u_n) = (j_1, a_1), \dots, (j_n, a_n)\}} \Psi_P(u_1 \dots u_n)\\
& = \PP_{\text{ind}}(l \circ \rho_{\pi_2}(u_1 \dots u_n) = a_1 \dots a_n)
\end{align*}
Hence for any $f : [0,1] \rightarrow [0,1]$
\begin{equation*}
\int_{\Sigma^n} f(\|\pi_1 \Psi \|) d\PP_{\pi_2} = \int_{P^n} f(\| \Psi^* \|) d\PP_{\text{ind}}.
\end{equation*}

Consider the bottom connected components $C_1, \dots, C_k \subseteq Q \times Q$ of the Markov chain defined by $\sum_{p \in P} \Psi^*(p)$. 

Let $d : Q \times Q \rightarrow Q \times Q$ be defined as $d(s_1, s_2) = (s_2, s_2)$.

\begin{lemma}\label{Qboundfordenominator}
Let $s$ be a starting state for an HMM $(Q, \Sigma, \Psi)$. Then $\lim_{n \rightarrow \infty} \frac1n \ln \|  (s, s) \Psi_n^* \|$ takes at most $|Q|$ values.
\end{lemma}

\begin{proof}
Let $P_1, \dots, P_K$ be the irreducible components of $\Psi^*$. Consider states $(s_1, s_2), (r_1, r_2) \in Q \times Q$. If there is a path from $(s_1, s_2)$ to $(r_1, r_2)$ then there is also a path from $(s_2, s_2)$ to $(r_2, r_2)$. Therefore for any end component $P_i$ it follows that the image $d(P_i) \subseteq P_j$ for some end component $P_j$ and so we may define a function $\rho : \{ P_1, \dots, P_K \} \rightarrow \{ P_1, \dots, P_K \}$ such that $\rho(P_i) = P_j$. Suppose $P_i, P_j$ have Lyapunov exponents $\lambda_i$ and $\lambda_j$ respectively. Let $\pi_1$ and $\pi_2$ be initial distributions such that the support of $\pi_1$ is in $P_i$ and the support of $\pi_2$ is in $P_j$ then the likelihood ratio $L_n = \frac{\| \pi_1 \Psi_n \|}{\| \pi_2 \Psi_n \|}$ converges to a limit in the set $[0,\infty)$ with respect to the measure $\PP_{\pi_2}$, the same limit as $\frac{\| (\pi_1, \pi_2) \Psi_n^* \|}{\| (\pi_2, \pi_2) \Psi_n^* \|}$ with respect to $\PP_\text{ind}$. Since both $\frac{1}{n} \ln \| (\pi_1, \pi_2) \Psi_n^* \|$ and $\frac{1}{n} \ln \| (\pi_2, \pi_2) \Psi_n^* \|$ converge almost surely in the set $[-\infty, 0]$ to $\lambda_i$ and $\lambda_j$ respectively, $\frac{1}{n} \ln \frac{\| \pi_1 \Psi_n \|}{\| \pi_2 \Psi_n \|}$ converges in $[-\infty, 0]$ with respect to $\PP_{\pi_2}$. Therefore $\lambda_i \leq \lambda_j$.
Now consider $\lim_{n \rightarrow \infty} \frac1n \ln \|  (s, s) \Psi_n^* \|$ whose possible limits is bounded by $|Q|^2$. Suppose for some word $w \in P^n$ the support of $(s, s) \Psi_n^*$ intersects an irreducible component $P_i$. Then it must also intersect $P_j$. Since $\lambda_i \leq \lambda_j$ it follows that $\lim_{n \rightarrow \infty} \frac1n \ln \|  (s, s) \Psi_n^* \| \geq \lambda_j$. Since the image $\rho \{P_1, \dots, P_K \} \leq |Q|$ it follows that $\lim_{n \rightarrow \infty} \frac1n \ln \|  (s, s) \Psi_n^* \|$ takes at most $|Q|$ values.
\end{proof}

\begin{lemma}
Let $\pi_1$ and $\pi_2$ be initial distributions of an HMM $(Q, \Sigma, \Psi)$. Then the likelihood exponent $\lim_{n \rightarrow \infty} \frac1n \ln L_n$ takes at most $|Q|^2$ values with respect to measure $\PP_{\pi_2}$ almost surely.
\end{lemma}

\begin{proof}
We may instead consider a bound on
\begin{equation*}
\lim_{n \rightarrow \infty} \frac{1}{n} \ln \frac{\| (\pi_1, \pi_2) \Psi_n^* \|}{\| (\pi_2, \pi_2) \Psi_n^* \|} = \lim_{n \rightarrow \infty} \frac1n \ln \| (\pi_1, \pi_2) \Psi_n^*\| - \lim_{n \rightarrow \infty} \frac1n \ln \| (\pi_2, \pi_2) \Psi_n^*\|
\end{equation*}
with respect to the $\PP_\text{ind}$ measure. Let $P_1, \dots, P_K$ be the irreducible components of $\Psi^*$ and partition $\Sigma^\omega$ into $W_1, \dots, W_K$ such that $\lim_{n \rightarrow \infty} \frac1n \ln \| (\pi_1, \pi_2) \Psi_n^*\| = \lambda_k$ for almost all $w \in W_k$. By ordering the $lambda_k$ as necessary, for all $w \in W_1$, there is an $r \leq K$ such that $\supp \Big( (\pi_1, \delta_q)\Psi^*(w) \Big)$ intersects all states in $\bigcup_{i = 1}^r P_i$ infinitely often. Then since a word $w$ exactly determines the state $r((\delta_q, \delta_q)\Psi^*(w))$, $(\delta_q, \delta_q)\Psi^*(w)$ hits all states in $\bigcup_{i = 1}^r \rho(P_i)$ infinitely often which implies that $\frac1n \ln \| (\delta_q, \delta_q)\Psi^*(w) \|$ converges to a constant on $W_1$. The same argument holds for $W_2, \dots, W_K$ and since $K \leq |Q|^2$ the lemma holds.

\end{proof}


\begin{center}
	\begin{tikzpicture}[scale=2.3,LMC style]
	\node[state] (s0s0) at (0,0) {$s_0$,$s_0$};
	\node[state] (s0s1) at (-1,0) {$s_0$,$s_1$};
	\node[state, fill=red] (s0s2) at (-1,-1) {$s_0$,$s_2$};
	\node[state] (s1s0) at (1,0) {$s_1$,$s_0$};
	\node[state, fill=green] (s1s1) at (0,1) {$s_1$,$s_1$};
	\node[state, fill=red] (s1s2) at (1,1) {$s_1$,$s_2$};
	\node[state, fill=red] (s2s0) at (1,-1) {$s_2$,$s_0$};
	\node[state, fill=red] (s2s1) at (-1,1) {$s_2$,$s_1$};
	\node[state] (s2s2) at (0,-1) {$s_2$,$s_2$};

	\path[->] (s0s1) edge [loop,out=200,in=160,looseness=10] node[pos=0.3,left] {$\frac13 [0,1)$} (s0s1);
	\path[->] (s1s0) edge [loop,out=20,in=340,looseness=10] node[pos=0.3,right] {$1 [0,\frac13)$} (s1s0);
	\path[->] (s2s2) edge [loop,out=250,in=290,looseness=10] node[pos=0.35,below] {$\frac12 [0, \frac12)$} (s2s2);
	\path[->] (s0s0) edge [loop,out=305,in=345,looseness=10] node[pos=0.4,right] {$\frac13 [0,\frac13)$} (s0s0);
	\path[->] (s1s1) edge [loop,out=70,in=110,looseness=10] node[pos=0.4,above] {$1 [0,1)$} (s1s1);
	
	\path[->] (s0s0) edge node[above,pos=0.4] {$\frac13 [\frac13, \frac23)$} (s0s1);
	\path[->] (s0s0) edge node[right,pos=0.8] {$\frac13 [\frac13, \frac23)$} (s2s1);
	\path[->] (s0s0) edge node[right,pos=0.6] {$\frac13 [\frac13, \frac23)$} (s1s1);
	\path[->] (s0s0) edge node[above,pos=0.6] {$\frac13 [0, \frac13)$} (s1s0);
	\path[->] (s0s0) edge node[left,pos=0.9] {$\frac13 [\frac23, 1)$} (s1s2);
	\path[->] (s0s0) edge node[left,pos=0.4] {$\frac13 [\frac23, 1)$} (s0s2);
	
	\path[->] (s0s1) edge node[left,pos=0.9] {$\frac13 [0, 1)$} (s2s1);
	\path[->] (s2s2) edge node[below,pos=0.4] {$\frac12 [0, \frac12)$} (s0s2);
	\path[->] (s1s0) edge node[right,pos=0.8] {$1 [\frac23, 1)$} (s1s2);
	\path[->] (s2s2) edge node[below,pos=0.6] {$\frac12 [\frac12, 1)$} (s2s0);
	
	\path[->] (s0s0) edge[out=290, in=70] node[right,pos=0.6] {$\frac13 [\frac23, 1)$} (s2s2);
	\path[->] (s2s2) edge[out=110, in=250] node[left,pos=0.2] {$\frac12 [\frac12, 1)$} (s0s0);
	
	\path[->] (s0s1) edge[loop, out=135, in=135, looseness=2.6] node[left,pos=0.4] {$\frac13 [0, 1)$} (s1s1);
	\path[->] (s1s0) edge[loop, out=45, in=45, looseness=2.6] node[right,pos=0.4] {$1 [\frac13, \frac23)$} (s1s1);
	\end{tikzpicture}
\end{center}

\subsection{Deterministic Chains}

A HMM $(Q, \Sigma, \Psi)$ is deterministic if for all $a \in \Sigma$, all rows of $\Psi(a)$ contain exactly one non-zero entry. If $q$ is a starting state, for any word in $w \in \Sigma$ the vector $\delta_q \Psi(w)$ contains at most one non-zero entry.

\begin{lemma}\label{determcrossprod}
Let $\Psi : \Sigma \rightarrow [0,1]^{Q \times Q}$ be deterministic, then $\Psi^* : P \rightarrow [0,1]^{(Q \times Q) \times (Q \times Q)}$ is deterministic.
\end{lemma}

\begin{proof}
Fix $p \in P$ then by definition of $p$, for all $i_1, j_1 \in Q$ there is exactly one $j_2 \in Q$ such that $r \circ \rho_{j_1} = j_2$ and at most one $i_2$ such that $\Psi(l \circ \rho_{j_1}(p))_{i_1, i_2} > 0$ since $\Psi$ is deterministic.
\end{proof}

\begin{proposition}
Let $(Q, \Sigma, \Psi)$ be a deterministic, irreducible cocycle then we may compute a number $\lambda \in [-\infty, 0]$ in time polynomial in $|Q|$ such that for any two starting state $q$ and independent producer of letters $\PP_{\text{ind}}$
\begin{equation*}
\PP_{\text{ind}} \Big( \lim_{n \rightarrow \infty} \frac1n \ln (e_q \Psi_n \1^T) = \lambda \Big) = 1.
\end{equation*}
\end{proposition}

\begin{proof}
By Protasov's theorem, such a $\lambda$ exists. In the case that for some $a \in \Sigma$ there is a zero row in $\Psi(a)$, $\lambda = -\infty$ since the irreducibility property guarantees we hit every state infinitely often. In the case that for all $a \in \Sigma$, all rows in $\Psi(a)$ contain at least one non-zero element we may conclude that $\lambda > -\infty$ since every word is producible from starting state $q$. This condition can be checked in $O(|Q|^2)$ time. In the case that $\lambda > -\infty$ consider the random sequence $i_1, \dots, i_n$ defined by $\Psi_{i_{k - 1}, i_k}(a_k) > 0$ for $k \geq 2$ and $i_1 = q$. Then write $r_k = \Psi_{i_{k - 1}, i_k}(a_k)$. The sequences $i_1, \dots, i_n$ and $r_1, \dots, r_n$ are well defined because $\Psi$ is deterministic and for all $a \in \Sigma$ there are no non-zero rows. Since $\Psi_\text{min} \leq \frac1n \ln (e_q \Psi_n \1^T) \leq \Psi_\text{max}$ by the dominated convergence theorem,
\begin{align*}
\lim_{n \rightarrow \infty} \frac1n \ln (\delta_q \Psi_n \1^T) & = \lim_{n \rightarrow \infty} \EE_{\text{ind}} [ \frac1n \ln (e_q \Psi_n \1^T) ] \\
& =  \lim_{n \rightarrow \infty} \EE_{\text{ind}} [ \frac1n \ln \big( r_1(a_1) \dots r_n(a_n)\big) ] \\
& =  \lim_{n \rightarrow \infty} \EE_{\text{ind}} [ \frac1n \sum_{k = 1}^n \ln r_k(a_k) ] \\
& =  \lim_{n \rightarrow \infty}  \frac1n \sum_{k = 1}^n \EE_{\text{ind}} [ \ln r_k(a_k) ]. \\
\end{align*}
The random variable $\ln r_k(a_k)$ depends only on $i_k$ and $a_k$. We may construct a Markov chain $M_{i,j}$ on $Q$ by taking $M_{i,j} = \sum_{a \in \Sigma} \delta_{\Psi(a)_{i,j} > 0} \PP(a)$ which is possible in $O(|Q|^2 |\Sigma|)$ time. Assume $M$ has period $p$ then writing $k = sp + r$, $e_q M^{sp + r}$ converges to the vector $\mu_r$, and $i_k$ has a distribution given by $e_q M^n$. It follows that

\begin{align*}
\lim_{n \rightarrow \infty} \frac{1}{n}\sum_{k = 1}^n \EE[\ln r_k(a_k)] &= \lim_{s \rightarrow \infty} \frac{1}{sp}\sum_{s = 0}^n \sum_{r = 1}^p \EE[\ln r_{sp + r}(a_{sp + r})] \\
& = \frac{1}{p}  \sum_{r = 1}^p \lim_{s \rightarrow \infty} \frac{1}{s}\sum_{s = 0}^n \EE[\ln r_{sp + r}(a_{sp + r})] \\
& =  \frac{1}{p}  \sum_{r = 1}^p \sum_{i = 1}^{|Q|} (\mu_r)_i \ln \Psi) .\\
\end{align*}  



\end{proof}


\begin{proposition}
Let $(Q, \Sigma, \Psi)$ be a deterministic HMM and let $\pi_1, \pi_2$ be initial distributions, then we may compute a set of numbers $\{\lambda_1, \dots, \lambda_K \}$ where $k \leq |Q|$ and a set of probabilities $\{ p_1, \dots, p_K \}$ such that
\begin{equation*}
\PP_{\pi_2} \Big( \lim_{n \rightarrow \infty} \frac1n \ln (\pi_1 \Psi_n \1^T) = \lambda_k \Big) = p_k  
\end{equation*}
in time polynomial $|Q|$

\end{proposition}

\begin{proof}

\end{proof}







\begin{theorem}\textsc{\textbf{Oseledet's Theorem}}
	Let $C_n : \Sigma^\omega \rightarrow \RR^{r \times r}$ satisfy
	\begin{itemize}
		\item $C_0(w) = I \quad \forall w \in \Sigma^\omega$
		\item $C_{n + m}(w) = C_{m}(\LL^n(w))C_n(w).$
	\end{itemize}
	Then, there exists a finite set $L = \{\lambda_1, \dots, \lambda_k\} \subset \RR$ with $k \leq r$ such that for $u \in \RR^r$ we have $\lim_{n \rightarrow \infty} \frac{1}{n} \ln \|u C_n(w)\| \in L.$ 
\end{theorem}


\begin{theorem}\textsc{\textbf{Protasov's Theorem}}
TBA
\end{theorem}






\subsection{Ergodicity}
Let $(\Sigma^\omega, \GG^*)$ be a measurable space of infinite words. The left-shift operator $\LL : \Sigma^\omega \rightarrow \Sigma^\omega$ is given as $\LL(w)_i = w_{i + 1}$. We say that a measure $\PP$ on $(\Sigma^\omega, \GG^*)$ is ergodic for $\LL$ if the following two conditions hold
\begin{itemize}
\item $\PP(\LL^{-1}(E)) = \PP(E) \quad \forall E \in \GG^*$
\item $\lim_{k \rightarrow \infty} \frac{1}{k} \sum_{i = 0}^{k  - 1}\PP_\mu(\LL^{-i}(A) \cap B) = \PP_\mu(A) \PP_\mu(B) \quad \forall A, B \in \GG^*$.
\end{itemize}

\begin{lemma}\label{markovergodic}
Let $M$ be an irreducible stochastic matrix, then $\lim_{k \rightarrow \infty} \frac{1}{k} \sum_{i = 0}^{k - 1} M^i = \1^T \mu$ where $\mu$ is the stationary distribution of $M$.
\end{lemma}





\begin{lemma}\label{ergodichmmleftshift}
Let $(Q, \Sigma, \Psi)$ be a connected HMM and let $\mu$ be an stationary distribution for $\int_\Sigma \Psi d\lambda$ then $\PP_\mu$ is ergodic for $\LL$.
\end{lemma}

\begin{proof}
For both conditions by \cite[p.~20, p.~41]{wal81} it suffices to check just cylinder sets. Let $n \in \NN$ and $E \in \Sigma^n$. Clearly, 
\[\PP_\mu(\LL^{-1}(E)) = \mu \int_{\Sigma}\Psi d\lambda \int_{E}\Psi d\lambda^n \1^T = \mu \int_{E}\Psi d\lambda^n \1^T = \PP_\mu(E).\]
And similarly, let $A, B \in \Sigma^n$ then
\begin{align*}
\lim_{k \rightarrow \infty} \frac{1}{k} \sum_{i = 0}^{k  - 1}\PP_\mu(\LL^{-i}(A) \cap B) & = \lim_{k \rightarrow \infty} \frac{1}{k} \sum_{i = n}^{k  - 1}\PP_\mu(\LL^{-i}(A) \cap B) \\
& = \lim_{k \rightarrow \infty} \frac{1}{k} \sum_{i = n}^{k  - 1} \mu \int_{B}\Psi d\lambda^n \int_{\Sigma}\Psi d\lambda^{i - n} \int_{A}\Psi d\lambda^n \1^T \\
& = \mu \int_{B}\Psi d\lambda^n \Big[ \lim_{k \rightarrow \infty} \frac{1}{k} \sum_{i = n}^{k  - 1} \Big( \int_{\Sigma}\Psi d\lambda \Big)^{i - n} \Big] \int_{A}\Psi d\lambda^n \1^T \\
& = \mu \int_{B}\Psi d\lambda^n \1^T \mu \int_{A}\Psi d\lambda^n \1^T \\
& = \PP_\mu(A)\PP_\mu(B)
\end{align*}
where the fourth equality follows by \Cref{markovergodic}.
\end{proof}

\begin{theorem}\textsc{\textbf{Kingman's Subadditive Ergodic Theorem}}
Let $(\Sigma^\omega, \GG^*, \mu)$ a probability space on infinite words such that $\mu$ is an ergodic measure for $\LL$. Further, let $X_n$ be a sequence of integrable random variables on this space such that $X_{n+m}(w) \leq X_{n}(w) + X_{m}(\LL^n w)$. Then $\lim_{n \rightarrow \infty} \frac{X_n}{n} $ converges $\mu$-a.e. to a constant.
\end{theorem}

\begin{corollary}\label{kingmancor}
Let $M = (Q, \Sigma, \Psi)$ be an HMM and suppose $\mu$ is an ergodic measure for $\LL$. Then, $\frac{1}{n} \ln \| \Psi_n \|$ converges $\mu$-a.e. to a constant.
\end{corollary}

\begin{proof}
$\LL$ is ergodic for measure $\mu$ by \Cref{ergodichmmleftshift}. Writing $w_n$ for the $n$-th letter of the infinite word $w$ and $X_n = \ln \| \Psi(w_1 \cdots w_{n}) \|$ it follows that $X_n$ is integrable and

\begin{align*}
X_{n + m}(w) = \ln \| \Psi(w_1 \cdots w_{n + m}) \| & \leq \ln \| \Psi(w_1 \cdots w_n) \| \|\Psi(w_{n+1} \cdots w_{n + m}) \| \\
& = \ln \| \Psi(w_1 \cdots w_n) \| + \ln \|\Psi(w_{n+1} \cdots w_{n + m}) \| \\
& = X_n(w) + X_{m}(T^n w).  
\end{align*}
\end{proof}



We say a matrix valued function $\Psi$ is deterministic if for all $a \in \Sigma$ there is a permutation matrix $M \in \{0,1\}^{|Q| \times |Q|}$ such that  $\Psi(a) \leq M$.



\begin{proposition}\label{computablelyaps}
if $(Q, \Sigma, \Psi)$ is a deterministic and connected HMM and $\mu$ is an stationary distribution of $\int_{\Sigma} \Psi$ then $\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| \pi \Psi_n \|$ exists $\PP_{\mu}$-a.s and is computable in polynomial time.
\end{proposition}

\begin{proof}
The existence of the limit is due to (ADD REF). Since $\supp~ \Psi(a)$ is a permutation matrix there is a function $f_a : [Q] \rightarrow (0,1]$ such that $f_a(j) = \Psi(a)_{l,j} > 0$ for some $l \in [Q]$.

Now consider the Markov chain $M^*$ and the random variable on the state space of $M^*$ defined as $X(i,j,a) = f_a(i)$. Writing the random sequence of states of $M^*$ as $(i_k, j_k, a_k)$ we have 
\[\frac{1}{n} \ln \| \pi \Psi_n \| = \frac{1}{n}\sum_{k = 1}^n \ln X(i_k, j_k, a_k)\]
Further since $X(i_k, j_k, a_k)$ takes values in a finite set $\frac{1}{n}\sum_{k = 1}^n \ln X(i_k, j_k, a_k)$ is bounded and by the dominated convergence theorem, 
\[\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| \pi \Psi_n \| = \lim_{n \rightarrow \infty} \frac{1}{n} \EE[ \ln \| \pi \Psi_n \| ] = \lim_{n \rightarrow \infty} \frac{1}{n}\sum_{k = 1}^n \EE[\ln X(i_k, j_k, a_k)].\]
Suppose $M^*$ has period $p$, then since $M^*$ is connected $(i_{sp + r}, j_{sp + r}, a_{sp + r})$ converges in distribution to $(i_r^*, j_r^*, a_r^*)$ as $s \rightarrow \infty$ which is the stationary distribution of $(M^*)^p$. Then $\EE[\ln X(i_{sp + r}, j_{sp + r}, a_{sp + r})] = \sum_{(i,j,a) \in Q^*} \ln X(i,j,a) \PP((i_{sp + r}, j_{sp + r}, a_{sp + r} = (i,j,a)) \rightarrow \EE[\ln X(i_r^*, j_r^*, a_r^*)]$ since $X$ takes values in a finite set.
\begin{align*}
\lim_{n \rightarrow \infty} \frac{1}{n}\sum_{k = 1}^n \EE[\ln X(i_k, j_k, a_k)] &= \lim_{s \rightarrow \infty} \frac{1}{sp}\sum_{s = 0}^n \sum_{r = 1}^p \EE[\ln X(i_{sp + r}, j_{sp + r}, a_{sp + r})] \\
& = \frac{1}{p}  \sum_{r = 1}^p \lim_{s \rightarrow \infty} \frac{1}{s}\sum_{s = 0}^n \EE[\ln X(i_{sp + r}, j_{sp + r}, a_{sp + r})] \\
& =  \frac{1}{p}  \sum_{r = 1}^p \EE[\ln X(i_r^*, j_r^*, a_r^*)].\\
\end{align*} 
we have that $p \leq |Q|$ and $\EE[\ln X(i_r^*, j_r^*, a_r^*)]$ is computable as the sum of at most $(|Q|^2 \times |\Sigma|)^2$ terms. 
\end{proof}

\begin{theorem}
Let $\Psi : \Sigma \rightarrow [0,1]^{|Q| \times |Q|}$ be a deterministic cocycle over $\Sigma$ and let $i \in Q$ be an initial state. One can determine in polynomial time a set of lyapunov exponents $\{\lambda_1, \dots, \lambda_K \}$ where $K \leq |Q|$ and for each $k \leq K$ the probabilities $\PP_{\text{ind}}(\lim_{n \rightarrow \infty} \frac{1}{n} \ln (\|\e_i \Psi^n\|) = \lambda_k)$. 
\end{theorem}

\begin{proof}(sketch)
Let $P_1, \dots, P_K$ be the end components of the graph of $\Psi$. The restriction $\Psi$ to any end component $P_k$ is an irreducible cocycle. Thus, by \Cref{computablelyaps} there are lyapunov exponents $\lambda_k$ computable in polynomial time such that conditioned on hitting $P_k$ $\lim_{n \rightarrow \infty} \frac{1}{n} \ln (\|\e_i \Psi^n\|) = \lambda_k$ almost surely. It remains to compute these hitting probabilities which can be done in polynomial time.
\end{proof}


\begin{lemma}
Let $(Q, \Sigma, \Phi)$ be an HMM and suppose that $\pi_1$ and $\pi_2$ are initial distributions such that $d(\pi_1, \pi_2) = 1$. Then there exists a $\gamma \in \QQ \cap (0,1]$ computable in polynomial in $|Q|$ such that for all $N \in \NN$. 
\begin{equation}\label{lyapboundassump}
\PP_{\pi_1}\Big(L_{2|Q|N} < 1\Big) - \PP_{\pi_2}\Big(L_{2|Q|N} < 1\Big) \geq 1 - 2\exp\big(-\gamma N \big).
\end{equation}
Further, 
\[\lim_{n \rightarrow \infty}\frac{1}{n} \ln L_n \leq -\frac{\gamma}{2|Q|} \quad \pi_1 \text{-a.s.}\]
\end{lemma}

\begin{proof}
By \Cref{lyapboundassump} it follows that $\PP_{\pi_2}(L_{2|Q|N} < 1) \leq 2 \exp(-\gamma N)$ and $\PP_{\pi_1}(L_{2|Q|N} \geq 1) \leq 2 \exp(-\gamma N)$. Let $\epsilon \in (0,1)$ and $W_{\epsilon} = \{\epsilon \leq L_{2|Q|N} < 1\}$.

\begin{align*}
\PP_{\pi_1}(\epsilon \leq L_{2|Q|N}) & \leq \PP_{\pi_1}(L_{2|Q|N} \geq 1) + \PP_{\pi_1}(W_{\epsilon}) \\
& \leq 2 \exp(-\gamma N) + \sum_{w \in W_\epsilon} \pi_1 \Psi(w) \1^T \\
& \leq 2 \exp(-\gamma N) + \frac{1}{\epsilon}\sum_{w \in W_\epsilon} \pi_2 \Psi(w) \1^T \\
& \leq 2 \exp(-\gamma N) + \frac{1}{\epsilon}\PP_{\pi_2} (L_{2|Q|N} < 1) \\
& \leq \big(2  + \frac{1}{\epsilon}\big)\exp(-\gamma N)
\end{align*}

Finally, let $\alpha \in (0,\frac{\gamma}{2|Q|})$ then by Fatou's lemma, 

\begin{align*}
\PP_{\pi_1}(-\alpha \leq \lim_{n \rightarrow \infty} \frac{1}{n} \ln L_n )  & \leq \PP_{\pi_1}(\liminf \{-\alpha \leq \frac{1}{n} \ln L_n \}) \\
& \leq \liminf \PP_{\pi_1}(-\alpha \leq \frac{1}{n} \ln L_n) \\
& \leq \liminf \PP_{\pi_1}(\exp(-2|Q|N\alpha) \leq L_{2|Q|N} ) \\
& = 0
\end{align*}
By choosing $\epsilon = \exp(-2|Q|N\alpha)$.

\end{proof}

\begin{lemma}
let $\alpha \in (0,1)$ then
\begin{equation*}
{n \choose \alpha n } \simeq \frac{\big(\alpha^\alpha (1 - \alpha)^{1 - \alpha} \big)^n}{\sqrt{2\pi n \alpha (1 - \alpha)}}
\end{equation*}
\end{lemma}

\begin{example}
Let $1 > \theta > \phi \geq \frac12$ and consider the disconnected HMM

\begin{center}
	\begin{tikzpicture}[scale=2.3,LMC style]
	\node[state] (s0) at (-1,0) {$s_0$};
	\node[state] (s1) at (1,0) {$s_1$};
	
	\path[->] (s0) edge [loop,out=200,in=160,looseness=10] node[pos=0.5,left] {$\theta a$} (s0);
	\path[->] (s0) edge [loop,out=20,in=340,looseness=10] node[pos=0.5,right] {$(1 - \theta) b$} (s0);
	\path[->] (s1) edge [loop,out=200,in=160,looseness=10] node[pos=0.5,left] {$\phi a$} (s1);
	\path[->] (s1) edge [loop,out=20,in=340,looseness=10] node[pos=0.5,right] {$(1 - \phi) b$} (s1);
	\end{tikzpicture}
\end{center}

By (INSERT THM) we may compute $\lim_{n \rightarrow \infty} \frac{1}{n} \ln L_n$ to be
\begin{equation*}
\phi \ln \theta + (1 - \phi) \ln (1 - \theta))  - \phi \ln \phi -  (1 - \phi) \ln (1 - \phi))
\end{equation*}
Suppose $w \in \Sigma^n$ contains $k$ instances of the letter $a$, then
$\PP_{s_0} (w) = \theta^{k} (1 - \theta)^{n - k}$ and $\PP_{s_1} (w) = \phi^{k} (1 - \phi)^{n - k}$ it follows that

\begin{align*}
\PP_{s_0}(w) \leq \PP_{s_1}(w) & \iff \theta^{k} (1 - \theta)^{n - k} \leq \phi^{k} (1 - \phi)^{n - k} \\
& \iff \Big( \frac{\theta (1 - \phi)}{\phi (1 - \theta)} \Big)^k \leq \Big( \frac{1 - \phi}{1 - \theta}\Big)^n \\
& \iff k \leq n ~\frac{\ln \frac{1 - \phi}{1 - \theta}}{\ln\frac{\theta(1 - \phi)}{\phi(1 - \theta)}} \\
\end{align*}

Let $\gamma = \frac{\ln \frac{1 - \phi}{1 - \theta}}{\ln\frac{\theta(1 - \phi)}{\phi(1 - \theta)}}$, then using the inequalities $1 - \frac{1}{x} \leq \ln x \leq x - 1$ it follows that 

\begin{equation*}
\gamma  = \frac{1}{1 + \ln \frac{\theta}{\phi} / \ln \frac{1 - \phi}{1 - \theta}} \leq \frac{1}{1 + (1 - \frac{\phi}{\theta})/ (\frac{1 - \phi}{1 - \theta} - 1)} = \frac{\frac{1 - \phi}{1 - \theta} -  1}{\frac{1 - \phi}{1 - \theta} - \frac{\phi}{\theta}} = \theta \frac{\theta - \phi}{\theta - \phi \theta - \phi + \phi\theta} = \theta.
\end{equation*}
Similarly $\phi \leq \gamma$.

The function $s \rightarrow \theta^s(1 - \theta)^{n - s}$ which by considering its derivative is increasing for $0 \leq s \leq \theta n$. Similarly, $s \rightarrow \phi^s(1 - \phi)^{n - s}$ is decreasing for $\phi n \leq s \leq n$.


\begin{align*}
\lim_{n \rightarrow \infty} \Big(\sum_{w \in \Sigma^n} \text{min}(w) \Big)^\frac{1}{n} & = \lim_{n \rightarrow \infty} \Big( \sum_{k = 0}^{\floor*{\gamma n}} {n \choose k} \theta^k(1 - \theta)^{n - k} + \sum_{k = \floor*{\gamma n}}^{n} {n \choose k} \phi^k(1 - \phi)^{n - k} \Big)^{\frac{1}{n}} \\
& = \gamma^\gamma (1 - \gamma)^{1 - \gamma} \max \{ \theta^\gamma (1 - \theta)^{1- \gamma}, \phi^\gamma (1 - \phi)^{1- \gamma}\}
\end{align*}




\end{example}

\begin{proposition}\textsc{\textbf{NFA language inclusion}}
NFA Language inclusion is PSPACE-complete.
\end{proposition}

\begin{corollary}
Let $(Q_1, \Sigma, \Psi_1)$ and $(Q_2, \Sigma, \Psi_2)$ be HMMs where the latter is connected and gives rise to probability measure $\PP_2$ on $\Sigma^\omega$. Assume both HMMs are represented as arrays of binary fractions.

Deciding whether $\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| \Psi_1(w) \| = -\infty$ $\PP_2$-a.s. is PSPACE-complete.
\end{corollary}

\begin{proof}
Suppose that for all $n \in \NN$ and $w \in \Sigma^n$ such that $\PP_2(w) > 0$ there exists $i,j \in [Q]$ such that $\Psi(w)_{i,j} > 0$. Then $\Psi(w)_{i,j} \geq \min_{i,j,a} \Psi(a)_{i,j}^n$ and hence $\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| \Psi(w) \| \geq \ln (\min_{i,j,a} \Psi(a)_{i,j})$. Since the limit is $\PP_2$-a.s. by \cref{kingmancor} it follows that $\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| \Psi_1(w) \| = -\infty$ if and only if there is some word $w \in \Sigma^*$ such that $\Psi_1(w) = 0$ and $\PP_2(w) > 0$. 

Consider an instance of the Mortality problem on $\{0,1\}$ matrices given by a matrix valued function $M : \Sigma \rightarrow  \{0,1\}^{N \times N}$. If $\sum_{a \in \Sigma, k \in [N]} M(a)_{i,k} = 0$ for some $i \in [N]$ then $M$ is mortal if and only if the sub-problem $M'$ is mortal where $M'$ is the matrix valued function $M$ but with the $i$th row and column removed. Therefore we may assume WLOG that $\sum_{a \in \Sigma, k \in [N]} M(a)_{i,k} > 0$ for all $i \in [N]$.  

Let $(\Psi_1(a))_{i,j} = M(a)_{i,j} / \sum_{a \in \Sigma, k \in [N]} M(a)_{i,k}$ and let $\Psi_2(a) = \frac{1}{|\Sigma|}$. Then $([N], \Sigma, \Psi_1)$ and $(\{1\}, \Sigma, \Psi_2)$ are valid HMMs and $\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| \Psi_1(w) \| = -\infty$ $\PP_2$-a.s. if and only if $M$ is mortal.

Now consider two HMMs $(Q_1, \Sigma, \Psi_1)$ and $(Q_2, \Sigma, \Psi_2)$ where the latter is connected. The transition function of an NFA can be represented as a matrix valued function $\Delta_i : \Sigma \rightarrow \{0,1\}^{|Q_i| \times |Q_i|}$. So consider the NFAs (for $i = 1, 2$) $N_i = (Q_i, \Sigma, \supp~ \Psi_i, q_i, Q)$ where $q_2$ is picked arbitrarily from $Q_2$. It follows that $\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| \Psi_1(w) \| = -\infty$ $\PP_2$-a.s. if and only if $\LL(M_1) \subseteq \LL(M_2)$ for all choices of $q_1 \in Q_1$.
\end{proof}

\subsubsection{Mortality Problem for 0-1 matrices}
Consider a cocycle with 0-1 entries $\Psi : \Sigma \rightarrow \{0,1\}^{N \times N}$. Decide whether there exists a word $w \in \Sigma^*$ such that $\Psi(w) = 0$.

\begin{theorem}
The Mortality problem for 0-1 matrices is PSPACE-complete.
\end{theorem}

The proof of this theorem is due to \cite{karasha09}. The problem is PSPACE-hard even when the cocycle forms an HMM by the following corollary

\begin{corollary}
The Mortality problem for HMMs is PSPACE-complete.
\end{corollary}

\begin{proof}
To show PSPACE-hardness, consider an instance of the mortality problem for a 0-1 matrix $M : \Sigma \rightarrow  \{0,1\}^{N \times N}$. If $\sum_{a \in \Sigma, k \in [N]} M(a)_{i,k} = 0$ for some $i \in [N]$ then $M$ is mortal if and only if the sub-problem $M'$ is mortal where $M'$ is the matrix valued function $M$ but with the $i$th row and column removed. Therefore we may assume WLOG that $\sum_{a \in \Sigma, k \in [N]} M(a)_{i,k} > 0$ for all $i \in [N]$. Let $\Psi(a)_{i,j} = M(a)_{i,j} / \sum_{a \in \Sigma, k \in [N]} M(a)_{i,k}$. Then $([N], \Sigma, \Psi_1)$ is a valid HMM and is mortal if and only if $M$ is mortal.

To show the problem is PSPACE-complete, given an instance of the mortality problem for an HMM $\Psi$, we construct an instance of the mortality problem for 0-1 cocycles simply by considering $\supp ~\Psi$.
\end{proof}

\subsubsection{Lyapunov distribution problem}
Consider an HMM $\Psi^* : \Sigma \rightarrow [0,1]^{N \times N}$ and a probability distribution $\PP_{\text{ind}}$ over $\Sigma^\omega$ where the letters are i.d.d. Let $P_1, \dots, P_K \subset [N]$ be the irreducible components of $\Psi^*$. By Protasov's theorem, upon restriction to a single irreducible component, $\frac{1}{n}\ln \| \Psi^* \restriction_{P_k} \|$ converges almost surely. Therefore there exists an assignment from irreducible end components to Lyapunov exponents $\rho : [K] \rightarrow \{\lambda_1, \dots, \lambda_n \}$.

Given an initial state $i$ compute a probability distribution $\mu$ over $[K]$ such that $\PP_{\text{ind}}^*(\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| e_i \Psi^*_n\| = \rho(k)) = \sum_{l : \rho(l) = \rho(k)} \mu(l)$.

\begin{theorem}
The Lyapunov distribution problem is PSPACE-hard. 
\end{theorem}

\begin{proof}
Suppose we have an algorithm to compute the probabilities in the Lyapunov distribution problem. Consider an instance of the Mortality problem with cocycle $\Psi : \Sigma \rightarrow [0,1]^{N \times N}$. Write $\Sigma = \{a_1, \dots, a_K \}$. We may compute $\Psi_{\text{min}}$ the smallest non zero value of all the matrices in $\Psi$ in time $O(K N^2)$. Consider uniformly picked letters in $\Sigma$ and a function $f : \Sigma \rightarrow [0,1]$, 

\begin{equation*}
f(a) = \begin{cases}
(\Psi_{\text{min}} / 2)^K & a = a_1 \\
(1 - (\Psi_{\text{min}} / 2)^K) / (K - 1) & a \neq a_1. \\	
\end{cases}
\end{equation*}
We construct a new cocycle $\Psi^* : \Sigma \rightarrow [0,1]^{(N + 2) \times (N + 2)}$ built in blocks:

\begin{equation*}
	\Psi^*(a) = \begin{pmatrix}
	0 & 1/2|\Sigma| & \1/2N|\Sigma|\\
	0 & f(a) & 0\\
	0 & 0 & \Psi(a)
	\end{pmatrix}
\end{equation*}
where $\1/2|\Sigma|$ is a row vector with $N$ elements which are identically $1/2|\Sigma|$. The second state is deterministic and also an end component so the following inequality due to \Cref{computablelyaps}  holds when $i = 2$


\begin{equation}\label{lam1ineq}
\lambda_1 = \lim_{n \rightarrow \infty} \frac{1}{n} \ln (\| e_2 \Psi^* \|) = \ln (\Psi_{\text{min}} / 2) + \frac{K - 1}{K} \ln \frac{1 - (\Psi_{\text{min}} / 2)^K}{(K - 1) } < \ln (\Psi_{\text{min}} / 2).
\end{equation}

Now consider the case $i = 1$ and suppose $\Psi$ has Lyapunov exponent $\lambda_2$ with respect to uniformly chosen letters. Then, 
\begin{align*}
\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| e_i \Psi^*\| &= \lim_{n \rightarrow \infty} \frac{1}{n} \ln \big( f^n + \| \Psi\| \big) \\
& = \max_{j = 1, 2}\{\lambda_1, \lambda_2\} \quad \text{a.s}.
\end{align*}
\Cref{lam1ineq} guarantees that $\lambda_1 \neq \lambda_2$. We may run our algorithm for the Lyapunov distribution problem on $\Psi^*$ with $i = 1$ which yields a distribution $p$ and $1 - p$ on the symbols $\lambda_1$ and $\lambda_2$ respectively such that $p \in \{0,1\}$. If $\Psi$ is not mortal then $\lambda_1 < \ln(\Psi_{\text{min}} / 2) < \ln \Psi_{\text{min}} < \lambda_2$ hence $p = 0$. If $\Psi$ is mortal then $-\infty = \lambda_2 < \lambda_1$ hence $p = 1$.
\end{proof}


\subsubsection{Lyapunov comparison problem}
Consider two irreducible cocycles $\Psi_1, \Psi_2 : \Sigma \rightarrow [0,1]^{N \times N}$ with lyapunov exponents $\lambda_1, \lambda_2$ respectively. Decide whether $\lambda_1 < \lambda_2$.

\subsubsection{Problem 1}
Consider a cocycle $\Psi^* : \Sigma \rightarrow \RR^{N \times N}$ and a probability distribution $\PP_{\text{ind}}$ over $\Sigma^\omega$ where the letters are i.d.d. Let $P_1, \dots, P_K \subset [N]$ be the irreducible components of $\Psi^*$ ordered in such a way that $\lambda : [K] \rightarrow [-\infty, 0)$ defined as $\lambda(k) = \lim_{n \rightarrow \infty} \frac{1}{n} \ln \|\Psi^*_n \restriction_{P_k}\|$ is monotonically increasing with respect to $k$. Given an initial state $i$ compute a probability distribution $\mu$ over $[K]$ such that $\PP_{\text{ind}}^*(\lim_{n \rightarrow \infty} \frac{1}{n} \ln \| e_i \Psi^*_n\| = \lambda(k)) = \mu(k)$.

\subsubsection{Problem 2}
Consider two NFAs $M_i = (Q, \Sigma, \Delta, q_0, F_i)$ for $i = 1, 2$ where $F_1$ and $F_2$ are disjoint and probability measure $\PP_{\text{ind}}$ on $\Sigma^\omega$ where the letters are i.i.d. Compute $\PP_{\text{ind}}(\mathcal{L}(M_1) \setminus \mathcal{L}(M_2))$.

\subsubsection{Problem 3}
Consider two DFAs $M_i = (Q, \Sigma, \delta, q_0, F_i)$ for $i = 1, 2$ where $F_1$ and $F_2$ are disjoint and probability measure $\PP_{\text{ind}}$ on $\Sigma^\omega$ where the letters are i.i.d. Compute $\PP_{\text{ind}}(\mathcal{L}(M_1) \setminus \mathcal{L}(M_2))$.

\subsubsection{Problem 3*}
Consider a DFA $D = (Q, \Sigma, \delta, q_0, F)$ and probability measure $\PP_{\text{ind}}$ on $\Sigma^\omega$ where the letters are i.i.d. Compute $\PP_{\text{ind}}(\mathcal{L}(D))$.

\subsubsection{Problem 4}
Let $(Q, M)$ be a Markov chain and let $F \subset Q$ be set of states. Given an initial state $i$, compute the hitting probability of $F$.


\begin{lemma}
Given an instance of problem 1 we can compute in logarithmic space at most N instances of problem 2 such that if we can solve problem 2 in PSPACE we can solve problem 1 in PSPACE.
\end{lemma}

\begin{proof}
Consider an instance of Problem 1. Let $Q = [N]$, $q \in \Delta(p, a) \iff \Psi^*(a)_{p,q} > 0$, $q_0 = i$. For $k = 1, \dots, K - 1$ we set $F_k^- = \bigcup_{j = 1}^k P_j$ and $F_k^+ = \bigcup_{j = k + 1}^K P_j$. After running our algorithm for Problem 2 on the NFAs $M_k^1 = (Q, \Sigma, \Delta, q_0, F_k^+)$ and $M_k^2 = (Q, \Sigma, \Delta, q_0, F_k^-)$ for each $k = 1, \dots, K - 1$, we obtain the probabilities $p_k = \PP_{\text{ind}}(\mathcal{L}(M_k^1) \setminus \mathcal{L}(M_k^2))$. Write $\mu_1 = p_1$ then compute $\mu_k = p_{k + 1} - p_k$ for each $k$. This gives the required probability distribution.
\end{proof}

\begin{lemma}
An instance of problem 3 can be reduced to an instance of problem 4 in time $O(|\Sigma| |Q|^2)$. 
\end{lemma}

\begin{proof}
Consider an instance of Problem 3. Recall that $\delta : Q \times \Sigma \rightarrow Q$. We construct an instance of Problem 4 where $Q$ and $F$ remain the same, $i = q_0$ and assigning the entries of $M$ by $M_{i,j} = \sum_{a \in \Sigma} \PP_{\text{ind}}(a) \chi_{\delta(i,a) = j}$. Computing the hitting probability of $F$ in the Markov chain gives $\PP_{\text{ind}}(\mathcal{L}(D))$ since the paths in the Markov chain hitting $F$ correspond to cylinder sets of words whose union is $\mathcal{L}(D)$.
\end{proof}

\begin{lemma}
Problem 4 can be computed in polynomial time.
\end{lemma}

\begin{proof}

\end{proof}


\bibliography{lyapunovhmm}

\appendix


\end{document}
  